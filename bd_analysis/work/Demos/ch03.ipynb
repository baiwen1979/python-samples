{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 Python + MR 实现Hadoop Streaming分区和二次排序\n",
    "---\n",
    "\n",
    "## 1. 实验目的\n",
    "\n",
    "基于MapReduce思想，编写分区（Partition）和二次排序（Secondary Sorting）程序\n",
    "\n",
    "## 2. 实验要求\n",
    "\n",
    "理解MapReduce编程思想，会用Python编写基于HadoopStreaming的MapReduce分区和二次排序程序，然后将其在本地测试并在Hadoop集群上执行并分析执行过程。\n",
    "\n",
    "## 3. 实验原理\n",
    "\n",
    "HadoopStreaming框架默认情况下会以’\\t’作为分隔符，将每行第一个’\\t’之前的部分作为key，其余内容作为value， \n",
    "如果没有’\\t’分隔符，则整行作为key；这个{key}\\t{value}对又作为该map对应的reduce的输入。\n",
    "\n",
    "HadoopStreaming提供以下参数来实现对map输出进行分区：\n",
    "\n",
    "* -D stream.map.output.field.separator: 指定分割key分隔符，默认是'\\t'\n",
    "* -D stream.num.map.output.key.fields: 用作key的字段 \n",
    "* -D map.output.key.field.separator: 指定key内部的分隔符 \n",
    "* -D num.key.fields.for.partition: 指定对key分出来的前几部分做partition而不是整个key\n",
    "\n",
    "MR默认会对键进行排序，然后有时候也需要对值进行排序，满足这种需求的一种方法是在reduce阶段排序收集过来的values，但如果有数量巨大的values可能就会导致内存溢出等问题，这就是二次排序的应用场景--将对值的排序也安排到MR计算过程之中，而非单独来做。\n",
    "\n",
    "二次排序就是首先按照第一字段排序，然后再对第一次字段相同的行按照第二字段排序，注意不能破坏第一次排序的结果。\n",
    "\n",
    "mapper的输出被分区到各个reducer之后，会有一步排序。默认是按照key做二次排序，如果key是多列组成，先按照第一列排序，第一列相同的，按照第二列排序，依此类推。\n",
    "\n",
    "如果需要自定义排序，这里要指定key内部的哪些元素用来做排序依据，是排字典序还是数字序，倒序还是正序。用来控制的参数是 mapred.text.key.comparator.options，\n",
    "可通过org.apache.hadoop.mapred.lib.KeyFieldBasedComparator来自定义使用key中的部分字段做比较。\n",
    "\n",
    "\n",
    "## 4. 实验步骤\n",
    "\n",
    "### 1）准备数据\n",
    "\n",
    "#### 需要按注册地（省）进行分区的车牌数据\n",
    "\n",
    "数据的每行分别为：车牌号,注册地,里程数\n",
    "\n",
    "```\n",
    "鲁V73930,鲁,549 \n",
    "黑ML1711,黑,235 \n",
    "鲁V75066,鲁,657 \n",
    "桂J73031,桂,900 \n",
    "晋M42387,晋,432 \n",
    "桂J73138,桂,456 \n",
    "晋M41665,晋,879 \n",
    "晋M42529,晋,790\n",
    "```\n",
    "\n",
    "#### 需要按注册地（省）分区并按序号二次排序的车牌数据\n",
    "\n",
    "数据的每行分别为：车牌号,注册地,序号,里程数\n",
    "```\n",
    "鲁V73930,鲁,2,549 \n",
    "黑ML1711,黑,1,235 \n",
    "鲁V75066,鲁,1,657 \n",
    "桂J73031,桂,1,900 \n",
    "晋M42387,晋,3,432 \n",
    "桂J73138,桂,2,456 \n",
    "晋M41665,晋,2,879 \n",
    "晋M42529,晋,1,790\n",
    "鲁V75530,鲁,3,569\n",
    "```\n",
    "### 2）编写程序\n",
    "\n",
    "#### 1. 分区（Partition）\n",
    "\n",
    "##### mapper（plate_partition_mapper.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "def main():    \n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        row = line.split(\",\")\n",
    "        if len(row) >= 3:\n",
    "            plate  = row[0]   # 车牌号\n",
    "            province = row[1] # 注册地\n",
    "            mile = row[2]     # 里程\n",
    "            print(province + \",\" + plate + \"\\t\" + mile) \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reducer（plate_partition_reducer.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\t0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    prov = None\n",
    "    sum_mile = 0\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        row, mile = line.split(\"\\t\")\n",
    "        mile = int(mile)\n",
    "        if prov == None:\n",
    "            prov = row[0].split(\",\")[0]\n",
    "            sum_mile = mile\n",
    "        else:\n",
    "            if prov == row[0].split(\",\")[0]:\n",
    "                # 相同组\n",
    "                sum_mile += mile\n",
    "            else:\n",
    "                # 不同组，输出上一组数据\n",
    "                print('{0}\\t{1}'.format(prov, sum_mile))\n",
    "                sum_mile = mile\n",
    "                prov = row[0].split(\",\")[0]\n",
    "\n",
    "    print('{0}\\t{1}'.format(prov, sum_mile))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 二次排序（Secondary Sorting）\n",
    "\n",
    "##### mapper（plate_secsort_mapper.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        row = line.split(\",\")\n",
    "        if len(row) >= 3:\n",
    "            plate  = row[0]   # 车牌号\n",
    "            province = row[1] # 注册地\n",
    "            order = row[2]    # 序号\n",
    "            mile = row[3]     # 里程\n",
    "            print(\",\".join([province, plate, order, mile]))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reducer（plate_secsort_reducer.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # 几乎什么都不用做\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        print line\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3）本地测试\n",
    "\n",
    "#### 1. 测试分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲁V73930,鲁,549 \r\n",
      "黑ML1711,黑,235 \r\n",
      "鲁V75066,鲁,657 \r\n",
      "桂J73031,桂,900 \r\n",
      "晋M42387,晋,432 \r\n",
      "桂J73138,桂,456 \r\n",
      "晋M41665,晋,879 \r\n",
      "晋M42529,晋,790"
     ]
    }
   ],
   "source": [
    "!cat data/plates_data1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "桂,桂J73031\t900\r\n",
      "桂,桂J73138\t456\r\n",
      "晋,晋M41665\t879\r\n",
      "晋,晋M42387\t432\r\n",
      "晋,晋M42529\t790\r\n",
      "黑,黑ML1711\t235\r\n",
      "鲁,鲁V73930\t549\r\n",
      "鲁,鲁V75066\t657\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/plates_data1.txt | \\\n",
    "python input_files/plate_partition_mapper.py | \\\n",
    "sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "桂\t1356\r\n",
      "晋\t2101\r\n",
      "黑\t235\r\n",
      "鲁\t1206\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/plates_data1.txt | \\\n",
    "python input_files/plate_partition_mapper.py | \\\n",
    "sort | \\\n",
    "python input_files/plate_partition_reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 测试二次排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲁V73930,鲁,2,549 \r\n",
      "黑ML1711,黑,1,235 \r\n",
      "鲁V75066,鲁,1,657 \r\n",
      "桂J73031,桂,1,900 \r\n",
      "晋M42387,晋,3,432 \r\n",
      "桂J73138,桂,2,456 \r\n",
      "晋M41665,晋,2,879 \r\n",
      "晋M42529,晋,1,790\r\n",
      "鲁V75530,鲁,3,569"
     ]
    }
   ],
   "source": [
    "!cat data/plates_data2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "桂,桂J73031,1,900\r\n",
      "桂,桂J73138,2,456\r\n",
      "晋,晋M41665,2,879\r\n",
      "晋,晋M42387,3,432\r\n",
      "晋,晋M42529,1,790\r\n",
      "黑,黑ML1711,1,235\r\n",
      "鲁,鲁V73930,2,549\r\n",
      "鲁,鲁V75066,1,657\r\n",
      "鲁,鲁V75530,3,569\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/plates_data2.txt | \\\n",
    "python input_files/plate_secsort_mapper.py | \\\n",
    "sort | \\\n",
    "python input_files/plate_secsort_reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 80\r\n",
      "drwxr-xr-x  3 xiaobai  staff    96  8 13 13:02 \u001b[34mdata\u001b[m\u001b[m\r\n",
      "-rwxr-xr-x  1 xiaobai  staff   386  8 13 13:08 \u001b[31mplate_partition_mapper.py\u001b[m\u001b[m\r\n",
      "-rwxr-xr-x  1 xiaobai  staff   717  8 13 13:08 \u001b[31mplate_partition_reducer.py\u001b[m\u001b[m\r\n",
      "-rw-r--r--  1 xiaobai  staff   761  8 13 15:05 plate_partition_run.sh\r\n",
      "-rw-r--r--  1 xiaobai  staff   449  8 13 11:32 plate_secsort_mapper.py\r\n",
      "-rw-r--r--  1 xiaobai  staff   210  8 13 11:37 plate_secsort_reducer.py\r\n",
      "-rwxr-xr-x  1 xiaobai  staff   982  8 13 15:33 \u001b[31mplate_secsort_run.sh\u001b[m\u001b[m\r\n",
      "-rwxr-xr-x  1 xiaobai  staff   568  8 12 16:42 \u001b[31mstock_mapper.py\u001b[m\u001b[m\r\n",
      "-rwxr-xr-x  1 xiaobai  staff   758  8 12 16:43 \u001b[31mstock_reducer.py\u001b[m\u001b[m\r\n",
      "-rwxr-xr-x  1 xiaobai  staff   603  8 11 17:39 \u001b[31mword_count_mapper.py\u001b[m\u001b[m\r\n",
      "-rwxr-xr-x  1 xiaobai  staff  1144  8 12 12:08 \u001b[31mword_count_reducer.py\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l input_files/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4）集群运行\n",
    "\n",
    "#### 1. 运行分区程序\n",
    "##### 编写启动脚本（plate_partition_run.sh）\n",
    "\n",
    "由于提交MapReduce作业的命令非常很长，参数很多，编写Shell启动脚本是一种高效的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "EXEC_PATH=$(dirname \"$0\")\r\n",
      "HP_CMD=hadoop\r\n",
      "JAR_PACKAGE=/usr/hadoop-2.8.3/share/hadoop/tools/lib/hadoop-streaming-2.8.3.jar\r\n",
      "IN_PATH=/input/plates_data1.txt\r\n",
      "OUT_PATH=/output/plates_part\r\n",
      "MAP_FILE=${EXEC_PATH}/plate_partition_mapper.py\r\n",
      "RED_FILE=${EXEC_PATH}/plate_partition_reducer.py\r\n",
      "$HP_CMD fs -rm -r $OUT_PATH\r\n",
      "$HP_CMD jar $JAR_PACKAGE \\\r\n",
      "-D mapred.job.queue.name=bdev \\\r\n",
      "-D stream.map.input.ignoreKey=true \\\r\n",
      "-D map.output.key.field.separator=, \\\r\n",
      "-D num.key.fields.for.partition=1 \\\r\n",
      "-numReduceTasks 1 \\\r\n",
      "-input $IN_PATH \\\r\n",
      "-output $OUT_PATH \\\r\n",
      "-file $MAP_FILE \\\r\n",
      "-file $RED_FILE \\\r\n",
      "-mapper $MAP_FILE \\\r\n",
      "-reducer $RED_FILE \\\r\n",
      "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\r\n",
      "$HP_CMD fs -ls $OUT_PATH"
     ]
    }
   ],
   "source": [
    "!cat input_files/plate_partition_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec namenode \\\n",
    "chmod +x /input_files/plate_partition_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec namenode \\\n",
    "chmod +x /input_files/plate_partition_mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec namenode \\\n",
    "chmod +x /input_files/plate_partition_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop-streaming-2.7.1.jar\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec namenode \\\n",
    "ls /opt/hadoop-2.7.1/share/hadoop/tools/lib | \\\n",
    "grep 'stream'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 上传数据文件到 hadoop 集群并提交 MR Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./input_files/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./data/plates_data1.txt ./input_files/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec namenode \\\n",
    "hadoop fs -mkdir /input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2019-08-13 05:03 /input\r\n",
      "drwxr-xr-x   - root supergroup          0 2019-08-11 12:20 /rmstate\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec namenode \\\n",
    "hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec namenode \\\n",
    "hadoop fs -put /input_files/data/plates_data1.txt /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 root supergroup        150 2019-08-13 05:05 /input/plates_data1.txt\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec namenode \\\n",
    "hadoop fs -ls /input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker exec namenode /input_files/plate_partition_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp ./input_files/data/plates_data1.txt master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp ./input_files/plate_partition_mapper.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp ./input_files/plate_partition_reducer.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp ./input_files/plate_partition_run.sh master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/input/plates_data1.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec master \\\n",
    "hadoop fs -put /root/plates_data1.txt /input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/13 12:03:19 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "Deleted /output/plates_part\n",
      "19/08/13 12:03:20 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/root/plate_partition_mapper.py, /root/plate_partition_reducer.py] [] /tmp/streamjob3031390533179335376.jar tmpDir=null\n",
      "19/08/13 12:03:22 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/08/13 12:03:22 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/08/13 12:03:22 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "19/08/13 12:03:22 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/08/13 12:03:22 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/08/13 12:03:22 INFO Configuration.deprecation: mapred.job.queue.name is deprecated. Instead, use mapreduce.job.queuename\n",
      "19/08/13 12:03:22 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "19/08/13 12:03:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1201774250_0001\n",
      "19/08/13 12:03:23 INFO mapred.LocalDistributedCacheManager: Localized file:/root/plate_partition_mapper.py as file:/tmp/hadoop-root/mapred/local/1565697803279/plate_partition_mapper.py\n",
      "19/08/13 12:03:23 INFO mapred.LocalDistributedCacheManager: Localized file:/root/plate_partition_reducer.py as file:/tmp/hadoop-root/mapred/local/1565697803280/plate_partition_reducer.py\n",
      "19/08/13 12:03:23 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/08/13 12:03:23 INFO mapreduce.Job: Running job: job_local1201774250_0001\n",
      "19/08/13 12:03:23 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/08/13 12:03:23 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/08/13 12:03:23 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:03:23 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:03:23 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/08/13 12:03:24 INFO mapred.LocalJobRunner: Starting task: attempt_local1201774250_0001_m_000000_0\n",
      "19/08/13 12:03:24 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:03:24 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:03:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: Processing split: file:/input/plates_data1.txt:0+150\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: numReduceTasks: 2\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/08/13 12:03:24 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_partition_mapper.py]\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "19/08/13 12:03:24 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "19/08/13 12:03:24 WARN partition.KeyFieldBasedPartitioner: Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead\n",
      "19/08/13 12:03:24 WARN partition.KeyFieldBasedPartitioner: Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead\n",
      "19/08/13 12:03:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/13 12:03:24 INFO mapreduce.Job: Job job_local1201774250_0001 running in uber mode : false\n",
      "19/08/13 12:03:24 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/08/13 12:03:24 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:03:24 INFO streaming.PipeMapRed: Records R/W=8/1\n",
      "19/08/13 12:03:24 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:03:24 INFO mapred.LocalJobRunner: \n",
      "19/08/13 12:03:24 INFO mapred.MapTask: Starting flush of map output\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: Spilling map output\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: bufstart = 0; bufend = 144; bufvoid = 104857600\n",
      "19/08/13 12:03:24 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600\n",
      "19/08/13 12:03:25 INFO mapred.MapTask: Finished spill 0\n",
      "19/08/13 12:03:25 INFO mapred.Task: Task:attempt_local1201774250_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Records R/W=8/1\n",
      "19/08/13 12:03:25 INFO mapred.Task: Task 'attempt_local1201774250_0001_m_000000_0' done.\n",
      "19/08/13 12:03:25 INFO mapred.Task: Final Counters for attempt_local1201774250_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2297\n",
      "\t\tFILE: Number of bytes written=378399\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=144\n",
      "\t\tMap output materialized bytes=172\n",
      "\t\tInput split bytes=80\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=189267968\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=170\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Finishing task: attempt_local1201774250_0001_m_000000_0\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Starting task: attempt_local1201774250_0001_r_000000_0\n",
      "19/08/13 12:03:25 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:03:25 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:03:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:03:25 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2087a46a\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/13 12:03:25 INFO reduce.EventFetcher: attempt_local1201774250_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/13 12:03:25 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1201774250_0001_m_000000_0 decomp: 102 len: 106 to MEMORY\n",
      "19/08/13 12:03:25 INFO reduce.InMemoryMapOutput: Read 102 bytes from map-output for attempt_local1201774250_0001_m_000000_0\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 102, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->102\n",
      "19/08/13 12:03:25 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/13 12:03:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:03:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 86 bytes\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: Merged 1 segments, 102 bytes to disk to satisfy reduce memory limit\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: Merging 1 files, 106 bytes from disk\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/13 12:03:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:03:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 86 bytes\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_partition_reducer.py]\n",
      "19/08/13 12:03:25 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/08/13 12:03:25 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: Records R/W=5/1\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:03:25 INFO mapred.Task: Task:attempt_local1201774250_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:03:25 INFO mapred.Task: Task attempt_local1201774250_0001_r_000000_0 is allowed to commit now\n",
      "19/08/13 12:03:25 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1201774250_0001_r_000000_0' to file:/output/plates_part/_temporary/0/task_local1201774250_0001_r_000000\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Records R/W=5/1 > reduce\n",
      "19/08/13 12:03:25 INFO mapred.Task: Task 'attempt_local1201774250_0001_r_000000_0' done.\n",
      "19/08/13 12:03:25 INFO mapred.Task: Final Counters for attempt_local1201774250_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2631\n",
      "\t\tFILE: Number of bytes written=378535\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=106\n",
      "\t\tReduce input records=5\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=5\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=189267968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Finishing task: attempt_local1201774250_0001_r_000000_0\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Starting task: attempt_local1201774250_0001_r_000001_0\n",
      "19/08/13 12:03:25 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:03:25 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:03:25 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:03:25 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3003aec9\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/13 12:03:25 INFO reduce.EventFetcher: attempt_local1201774250_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/08/13 12:03:25 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1201774250_0001_m_000000_0 decomp: 62 len: 66 to MEMORY\n",
      "19/08/13 12:03:25 INFO reduce.InMemoryMapOutput: Read 62 bytes from map-output for attempt_local1201774250_0001_m_000000_0\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 62, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->62\n",
      "19/08/13 12:03:25 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/13 12:03:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:03:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 46 bytes\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: Merged 1 segments, 62 bytes to disk to satisfy reduce memory limit\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: Merging 1 files, 66 bytes from disk\n",
      "19/08/13 12:03:25 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/13 12:03:25 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:03:25 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "19/08/13 12:03:25 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 46 bytes\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_partition_reducer.py]\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:03:25 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:03:25 INFO mapred.Task: Task:attempt_local1201774250_0001_r_000001_0 is done. And is in the process of committing\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:03:25 INFO mapred.Task: Task attempt_local1201774250_0001_r_000001_0 is allowed to commit now\n",
      "19/08/13 12:03:25 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1201774250_0001_r_000001_0' to file:/output/plates_part/_temporary/0/task_local1201774250_0001_r_000001\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "19/08/13 12:03:25 INFO mapred.Task: Task 'attempt_local1201774250_0001_r_000001_0' done.\n",
      "19/08/13 12:03:25 INFO mapred.Task: Final Counters for attempt_local1201774250_0001_r_000001_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2819\n",
      "\t\tFILE: Number of bytes written=378630\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=66\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=3\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=17\n",
      "\t\tTotal committed heap usage (bytes)=189792256\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=29\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: Finishing task: attempt_local1201774250_0001_r_000001_0\n",
      "19/08/13 12:03:25 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/08/13 12:03:25 INFO mapreduce.Job:  map 100% reduce 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/13 12:03:26 INFO mapreduce.Job: Job job_local1201774250_0001 completed successfully\n",
      "19/08/13 12:03:27 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7747\n",
      "\t\tFILE: Number of bytes written=1135564\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=8\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=144\n",
      "\t\tMap output materialized bytes=172\n",
      "\t\tInput split bytes=80\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=172\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=17\n",
      "\t\tTotal committed heap usage (bytes)=568328192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=170\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=59\n",
      "19/08/13 12:03:27 INFO streaming.StreamJob: Output directory: /output/plates_part\n",
      "Found 3 items\n",
      "-rw-r--r--   1 root root          0 2019-08-13 12:03 /output/plates_part/_SUCCESS\n",
      "-rw-r--r--   1 root root         18 2019-08-13 12:03 /output/plates_part/part-00000\n",
      "-rw-r--r--   1 root root         17 2019-08-13 12:03 /output/plates_part/part-00001\n"
     ]
    }
   ],
   "source": [
    "!docker exec master \\\n",
    "/root/plate_partition_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "晋\t2101\r\n",
      "鲁\t1206\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec master \\\n",
    "hadoop fs -cat /output/plates_part/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "桂\t1356\r\n",
      "黑\t235\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec master \\\n",
    "hadoop fs -cat /output/plates_part/part-00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 运行二次排序\n",
    "\n",
    "##### 编写启动脚本（plate_secsort_run.sh）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "EXEC_PATH=$(dirname \"$0\")\r\n",
      "HP_CMD=hadoop\r\n",
      "JAR_PACKAGE=/usr/hadoop-2.8.3/share/hadoop/tools/lib/hadoop-streaming-2.8.3.jar\r\n",
      "IN_PATH=/input/plates_data2.txt\r\n",
      "OUT_PATH=/output/plates_secsort\r\n",
      "MAP_FILE=${EXEC_PATH}/plate_secsort_mapper.py\r\n",
      "RED_FILE=${EXEC_PATH}/plate_secsort_reducer.py\r\n",
      "$HP_CMD fs -rm -r $OUT_PATH\r\n",
      "$HP_CMD jar $JAR_PACKAGE \\\r\n",
      "-D mapred.job.queue.name=bdev \\\r\n",
      "-D stream.map.input.ignoreKey=true \\\r\n",
      "-D stream.map.output.field.separator=, \\\r\n",
      "-D stream.num.map.output.key.fields=3 \\\r\n",
      "-D map.output.key.field.separator=, \\\r\n",
      "-D num.key.fields.for.partition=1 \\\r\n",
      "-D mapred.output.key.comparator.class=\\\r\n",
      "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\r\n",
      "-D mapred.text.key.comparator.options=-k3,3nr \\\r\n",
      "-numReduceTasks 5 \\\r\n",
      "-input $IN_PATH \\\r\n",
      "-output $OUT_PATH \\\r\n",
      "-file $MAP_FILE \\\r\n",
      "-file $RED_FILE \\\r\n",
      "-mapper $MAP_FILE \\\r\n",
      "-reducer $RED_FILE \\\r\n",
      "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\r\n",
      "$HP_CMD fs -ls $OUT_PATH"
     ]
    }
   ],
   "source": [
    "!cat input_files/plate_secsort_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp data/plates_data2.txt master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp input_files/plate_secsort_mapper.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp input_files/plate_secsort_reducer.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp input_files/plate_secsort_run.sh master:/root/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 上传数据到 hadoop 集群并提交 MR Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master \\\n",
    "hadoop fs -put /root/plates_data2.txt /input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master \\\n",
    "chmod +x /root/plate_secsort_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/13 12:12:16 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "Deleted /output/plates_secsort\n",
      "19/08/13 12:12:17 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/root/plate_secsort_mapper.py, /root/plate_secsort_reducer.py] [] /tmp/streamjob6141640170064330830.jar tmpDir=null\n",
      "19/08/13 12:12:18 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/08/13 12:12:18 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/08/13 12:12:18 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "19/08/13 12:12:19 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/08/13 12:12:19 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/08/13 12:12:19 INFO Configuration.deprecation: mapred.job.queue.name is deprecated. Instead, use mapreduce.job.queuename\n",
      "19/08/13 12:12:19 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "19/08/13 12:12:19 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "19/08/13 12:12:19 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "19/08/13 12:12:19 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local234080687_0001\n",
      "19/08/13 12:12:20 INFO mapred.LocalDistributedCacheManager: Localized file:/root/plate_secsort_mapper.py as file:/tmp/hadoop-root/mapred/local/1565698340045/plate_secsort_mapper.py\n",
      "19/08/13 12:12:20 INFO mapred.LocalDistributedCacheManager: Localized file:/root/plate_secsort_reducer.py as file:/tmp/hadoop-root/mapred/local/1565698340046/plate_secsort_reducer.py\n",
      "19/08/13 12:12:20 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/08/13 12:12:20 INFO mapreduce.Job: Running job: job_local234080687_0001\n",
      "19/08/13 12:12:20 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/08/13 12:12:20 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/08/13 12:12:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:12:20 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:12:20 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/08/13 12:12:20 INFO mapred.LocalJobRunner: Starting task: attempt_local234080687_0001_m_000000_0\n",
      "19/08/13 12:12:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:12:20 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:12:20 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:12:20 INFO mapred.MapTask: Processing split: file:/input/plates_data2.txt:0+186\n",
      "19/08/13 12:12:20 INFO mapred.MapTask: numReduceTasks: 5\n",
      "19/08/13 12:12:20 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/08/13 12:12:20 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/08/13 12:12:20 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/08/13 12:12:20 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/08/13 12:12:20 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/08/13 12:12:20 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/08/13 12:12:20 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_secsort_mapper.py]\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "19/08/13 12:12:20 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "19/08/13 12:12:20 WARN partition.KeyFieldBasedPartitioner: Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead\n",
      "19/08/13 12:12:20 WARN partition.KeyFieldBasedPartitioner: Using deprecated num.key.fields.for.partition. Use mapreduce.partition.keypartitioner.options instead\n",
      "19/08/13 12:12:20 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: Records R/W=9/1\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: \n",
      "19/08/13 12:12:21 INFO mapred.MapTask: Starting flush of map output\n",
      "19/08/13 12:12:21 INFO mapred.MapTask: Spilling map output\n",
      "19/08/13 12:12:21 INFO mapred.MapTask: bufstart = 0; bufend = 180; bufvoid = 104857600\n",
      "19/08/13 12:12:21 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600\n",
      "19/08/13 12:12:21 INFO mapred.MapTask: Finished spill 0\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task:attempt_local234080687_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Records R/W=9/1\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task 'attempt_local234080687_0001_m_000000_0' done.\n",
      "19/08/13 12:12:21 INFO mapred.Task: Final Counters for attempt_local234080687_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1752\n",
      "\t\tFILE: Number of bytes written=378015\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=9\n",
      "\t\tMap output records=9\n",
      "\t\tMap output bytes=180\n",
      "\t\tMap output materialized bytes=228\n",
      "\t\tInput split bytes=80\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=201326592\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=206\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local234080687_0001_m_000000_0\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Starting task: attempt_local234080687_0001_r_000000_0\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:12:21 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:12:21 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@922022d\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/13 12:12:21 INFO reduce.EventFetcher: attempt_local234080687_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/08/13 12:12:21 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local234080687_0001_m_000000_0 decomp: 90 len: 94 to MEMORY\n",
      "19/08/13 12:12:21 INFO reduce.InMemoryMapOutput: Read 90 bytes from map-output for attempt_local234080687_0001_m_000000_0\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 90, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->90\n",
      "19/08/13 12:12:21 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "19/08/13 12:12:21 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/13 12:12:21 INFO mapreduce.Job: Job job_local234080687_0001 running in uber mode : false\n",
      "19/08/13 12:12:21 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 72 bytes\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merged 1 segments, 90 bytes to disk to satisfy reduce memory limit\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merging 1 files, 94 bytes from disk\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 72 bytes\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_secsort_reducer.py]\n",
      "19/08/13 12:12:21 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/08/13 12:12:21 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task:attempt_local234080687_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task attempt_local234080687_0001_r_000000_0 is allowed to commit now\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local234080687_0001_r_000000_0' to file:/output/plates_secsort/_temporary/0/task_local234080687_0001_r_000000\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Records R/W=4/1 > reduce\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task 'attempt_local234080687_0001_r_000000_0' done.\n",
      "19/08/13 12:12:21 INFO mapred.Task: Final Counters for attempt_local234080687_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2202\n",
      "\t\tFILE: Number of bytes written=378201\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=94\n",
      "\t\tReduce input records=4\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=4\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=12\n",
      "\t\tTotal committed heap usage (bytes)=201850880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=92\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local234080687_0001_r_000000_0\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Starting task: attempt_local234080687_0001_r_000001_0\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:12:21 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:12:21 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@28731cb8\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/13 12:12:21 INFO reduce.EventFetcher: attempt_local234080687_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/08/13 12:12:21 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local234080687_0001_m_000000_0 decomp: 46 len: 50 to MEMORY\n",
      "19/08/13 12:12:21 INFO reduce.InMemoryMapOutput: Read 46 bytes from map-output for attempt_local234080687_0001_m_000000_0\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 46, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->46\n",
      "19/08/13 12:12:21 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28 bytes\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merged 1 segments, 46 bytes to disk to satisfy reduce memory limit\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merging 1 files, 50 bytes from disk\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28 bytes\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_secsort_reducer.py]\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task:attempt_local234080687_0001_r_000001_0 is done. And is in the process of committing\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task attempt_local234080687_0001_r_000001_0 is allowed to commit now\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local234080687_0001_r_000001_0' to file:/output/plates_secsort/_temporary/0/task_local234080687_0001_r_000001\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task 'attempt_local234080687_0001_r_000001_0' done.\n",
      "19/08/13 12:12:21 INFO mapred.Task: Final Counters for attempt_local234080687_0001_r_000001_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2514\n",
      "\t\tFILE: Number of bytes written=378303\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce shuffle bytes=50\n",
      "\t\tReduce input records=2\n",
      "\t\tReduce output records=2\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=201850880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=52\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local234080687_0001_r_000001_0\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Starting task: attempt_local234080687_0001_r_000002_0\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:12:21 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:12:21 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ba0211a\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/13 12:12:21 INFO reduce.EventFetcher: attempt_local234080687_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/13 12:12:21 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local234080687_0001_m_000000_0 decomp: 68 len: 72 to MEMORY\n",
      "19/08/13 12:12:21 INFO reduce.InMemoryMapOutput: Read 68 bytes from map-output for attempt_local234080687_0001_m_000000_0\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 68, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->68\n",
      "19/08/13 12:12:21 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 50 bytes\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merged 1 segments, 68 bytes to disk to satisfy reduce memory limit\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merging 1 files, 72 bytes from disk\n",
      "19/08/13 12:12:21 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 50 bytes\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_secsort_reducer.py]\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: Records R/W=3/1\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:12:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task:attempt_local234080687_0001_r_000002_0 is done. And is in the process of committing\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task attempt_local234080687_0001_r_000002_0 is allowed to commit now\n",
      "19/08/13 12:12:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local234080687_0001_r_000002_0' to file:/output/plates_secsort/_temporary/0/task_local234080687_0001_r_000002\n",
      "19/08/13 12:12:21 INFO mapred.LocalJobRunner: Records R/W=3/1 > reduce\n",
      "19/08/13 12:12:21 INFO mapred.Task: Task 'attempt_local234080687_0001_r_000002_0' done.\n",
      "19/08/13 12:12:22 INFO mapred.Task: Final Counters for attempt_local234080687_0001_r_000002_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2798\n",
      "\t\tFILE: Number of bytes written=378447\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce shuffle bytes=72\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=3\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=201850880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=72\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local234080687_0001_r_000002_0\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: Starting task: attempt_local234080687_0001_r_000003_0\n",
      "19/08/13 12:12:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:12:22 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:12:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:12:22 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@40c88080\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/13 12:12:22 INFO reduce.EventFetcher: attempt_local234080687_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/08/13 12:12:22 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local234080687_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "19/08/13 12:12:22 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local234080687_0001_m_000000_0\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "19/08/13 12:12:22 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/13 12:12:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:22 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/13 12:12:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:22 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:22 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_secsort_reducer.py]\n",
      "19/08/13 12:12:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:12:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:12:22 INFO mapred.Task: Task:attempt_local234080687_0001_r_000003_0 is done. And is in the process of committing\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:22 INFO mapred.Task: Task attempt_local234080687_0001_r_000003_0 is allowed to commit now\n",
      "19/08/13 12:12:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_local234080687_0001_r_000003_0' to file:/output/plates_secsort/_temporary/0/task_local234080687_0001_r_000003\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "19/08/13 12:12:22 INFO mapred.Task: Task 'attempt_local234080687_0001_r_000003_0' done.\n",
      "19/08/13 12:12:22 INFO mapred.Task: Final Counters for attempt_local234080687_0001_r_000003_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2944\n",
      "\t\tFILE: Number of bytes written=378461\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=6\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=201850880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local234080687_0001_r_000003_0\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: Starting task: attempt_local234080687_0001_r_000004_0\n",
      "19/08/13 12:12:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/13 12:12:22 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/13 12:12:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/13 12:12:22 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3dd4bf2\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/13 12:12:22 INFO reduce.EventFetcher: attempt_local234080687_0001_r_000004_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/08/13 12:12:22 INFO reduce.LocalFetcher: localfetcher#5 about to shuffle output of map attempt_local234080687_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "19/08/13 12:12:22 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local234080687_0001_m_000000_0\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "19/08/13 12:12:22 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/13 12:12:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:22 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "19/08/13 12:12:22 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/13 12:12:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/13 12:12:22 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:22 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./plate_secsort_reducer.py]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/13 12:12:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/13 12:12:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/13 12:12:22 INFO mapred.Task: Task:attempt_local234080687_0001_r_000004_0 is done. And is in the process of committing\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/13 12:12:22 INFO mapred.Task: Task attempt_local234080687_0001_r_000004_0 is allowed to commit now\n",
      "19/08/13 12:12:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_local234080687_0001_r_000004_0' to file:/output/plates_secsort/_temporary/0/task_local234080687_0001_r_000004\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "19/08/13 12:12:22 INFO mapred.Task: Task 'attempt_local234080687_0001_r_000004_0' done.\n",
      "19/08/13 12:12:22 INFO mapred.Task: Final Counters for attempt_local234080687_0001_r_000004_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3084\n",
      "\t\tFILE: Number of bytes written=378475\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=6\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=201850880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local234080687_0001_r_000004_0\n",
      "19/08/13 12:12:22 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/08/13 12:12:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/08/13 12:12:22 INFO mapreduce.Job: Job job_local234080687_0001 completed successfully\n",
      "19/08/13 12:12:22 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15294\n",
      "\t\tFILE: Number of bytes written=2269902\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=9\n",
      "\t\tMap output records=9\n",
      "\t\tMap output bytes=180\n",
      "\t\tMap output materialized bytes=228\n",
      "\t\tInput split bytes=80\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9\n",
      "\t\tReduce shuffle bytes=228\n",
      "\t\tReduce input records=9\n",
      "\t\tReduce output records=9\n",
      "\t\tSpilled Records=18\n",
      "\t\tShuffled Maps =5\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=5\n",
      "\t\tGC time elapsed (ms)=12\n",
      "\t\tTotal committed heap usage (bytes)=1210580992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=206\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=232\n",
      "19/08/13 12:12:22 INFO streaming.StreamJob: Output directory: /output/plates_secsort\n",
      "Found 6 items\n",
      "-rw-r--r--   1 root root          0 2019-08-13 12:12 /output/plates_secsort/_SUCCESS\n",
      "-rw-r--r--   1 root root         80 2019-08-13 12:12 /output/plates_secsort/part-00000\n",
      "-rw-r--r--   1 root root         40 2019-08-13 12:12 /output/plates_secsort/part-00001\n",
      "-rw-r--r--   1 root root         60 2019-08-13 12:12 /output/plates_secsort/part-00002\n",
      "-rw-r--r--   1 root root          0 2019-08-13 12:12 /output/plates_secsort/part-00003\n",
      "-rw-r--r--   1 root root          0 2019-08-13 12:12 /output/plates_secsort/part-00004\n"
     ]
    }
   ],
   "source": [
    "!docker exec master \\\n",
    "/root/plate_secsort_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲁,鲁V75530,3\t569\r\n",
      "鲁,鲁V73930,2\t549\r\n",
      "鲁,鲁V75066,1\t657\r\n",
      "黑,黑ML1711,1\t235\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec master hadoop fs -cat /output/plates_secsort/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "桂,桂J73138,2\t456\r\n",
      "桂,桂J73031,1\t900\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec master hadoop fs -cat /output/plates_secsort/part-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "晋,晋M42387,3\t432\r\n",
      "晋,晋M41665,2\t879\r\n",
      "晋,晋M42529,1\t790\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec master hadoop fs -cat /output/plates_secsort/part-00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5）离线分析\n",
    "\n",
    "#### 1. 分区数据分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 下载分区文件到本地并合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master hadoop fs -cat /output/plates_part/part-00000 > \\\n",
    "./data/plates_part_result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master hadoop fs -cat /output/plates_part/part-00001 >> \\\n",
    "./data/plates_part_result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "晋\t2101\r\n",
      "鲁\t1206\r\n",
      "桂\t1356\r\n",
      "黑\t235\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/plates_part_result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分析与可视化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province</th>\n",
       "      <th>Total Miles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>晋</td>\n",
       "      <td>2101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>鲁</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>桂</td>\n",
       "      <td>1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>黑</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province  Total Miles\n",
       "0        晋         2101\n",
       "1        鲁         1206\n",
       "2        桂         1356\n",
       "3        黑          235"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/plates_part_result.txt', sep='\\t', names=('Province', 'Total Miles'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1183af908>"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 二此排序数据分析\n",
    "\n",
    "##### 下载并合并分区数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master hadoop fs -cat /output/plates_secsort/part-00000 > \\\n",
    "./data/plates_secsort_result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master hadoop fs -cat /output/plates_secsort/part-00001 >> \\\n",
    "./data/plates_secsort_result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master hadoop fs -cat /output/plates_secsort/part-00002 >> \\\n",
    "./data/plates_secsort_result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲁,鲁V75530,3\t569\r\n",
      "鲁,鲁V73930,2\t549\r\n",
      "鲁,鲁V75066,1\t657\r\n",
      "黑,黑ML1711,1\t235\r\n",
      "桂,桂J73138,2\t456\r\n",
      "桂,桂J73031,1\t900\r\n",
      "晋,晋M42387,3\t432\r\n",
      "晋,晋M41665,2\t879\r\n",
      "晋,晋M42529,1\t790\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/plates_secsort_result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 分析与可视化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    result = []\n",
    "    with open(filename) as f:\n",
    "        for row in f:\n",
    "            key, mile = row.split('\\t')\n",
    "            key = key.split(',')\n",
    "            mile = int(mile)\n",
    "            if len(key) >= 3:\n",
    "                province = key[0]\n",
    "                plate = key[1]\n",
    "                order = key[2]\n",
    "                result.append([province, plate, order, mile])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = read_file('./data/plates_secsort_result.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province</th>\n",
       "      <th>Plate</th>\n",
       "      <th>Order</th>\n",
       "      <th>Mile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鲁</td>\n",
       "      <td>鲁V75530</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>鲁</td>\n",
       "      <td>鲁V73930</td>\n",
       "      <td>2</td>\n",
       "      <td>549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>鲁</td>\n",
       "      <td>鲁V75066</td>\n",
       "      <td>1</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>黑</td>\n",
       "      <td>黑ML1711</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>桂</td>\n",
       "      <td>桂J73138</td>\n",
       "      <td>2</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>桂</td>\n",
       "      <td>桂J73031</td>\n",
       "      <td>1</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>晋</td>\n",
       "      <td>晋M42387</td>\n",
       "      <td>3</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>晋</td>\n",
       "      <td>晋M41665</td>\n",
       "      <td>2</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>晋</td>\n",
       "      <td>晋M42529</td>\n",
       "      <td>1</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province    Plate Order  Mile\n",
       "0        鲁  鲁V75530     3   569\n",
       "1        鲁  鲁V73930     2   549\n",
       "2        鲁  鲁V75066     1   657\n",
       "3        黑  黑ML1711     1   235\n",
       "4        桂  桂J73138     2   456\n",
       "5        桂  桂J73031     1   900\n",
       "6        晋  晋M42387     3   432\n",
       "7        晋  晋M41665     2   879\n",
       "8        晋  晋M42529     1   790"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(result, columns = ['Province','Plate','Order', 'Mile'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1182a9c88>"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEPNJREFUeJzt3X+QXWV9x/H3FzYQfgfDyoRd6q5j1NoKCjuESumkxrYJdBKmY2aCv4JSM6NQYilTtzpVnNoOzjClynScyRgVHRoUqkMKDkrBOEOp0Q1C+BGUgClZA7hGiI0QTOTbP+4Jrskme7a5d8/uk/drJrPnPOc553wvu3zuc59z77mRmUiSynVE0wVIkjrLoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVrqvpAgBOOeWU7Ovra7oMSZpWNmzY8LPM7B6v35QI+r6+PoaGhpouQ5KmlYj4nzr9nLqRpMIZ9JJUOINekgo3Jebox7J7926Gh4fZtWtX06V0xMyZM+nt7WXGjBlNlyKpcFM26IeHhznhhBPo6+sjIpoup60yk+3btzM8PEx/f3/T5Ugq3JSdutm1axezZ88uLuQBIoLZs2cX+2pF0tQyZYMeKDLk9yr5sUmaWqZ00EuSDt2UnaPfV9/g7W093pZrLhy3T0Twrne9iy9/+csA7Nmzhzlz5jBv3jxuu+021q5dyyOPPMLg4CBXX301xx9/PFdddVVb69ThpZ1/53X+xnV4mDZB34TjjjuOhx56iBdeeIFjjjmGO++8k56enpe3L168mMWLFzdYoSSNz6mbcSxatIjbb2+NstasWcPFF1/88rYvfvGLXH755fvt8/jjj7Nw4ULOPvtszj//fB599NFJq1eS9mXQj2PZsmXcdNNN7Nq1i40bNzJv3rxx91mxYgXXX389GzZs4Nprr+WDH/zgJFQqSWNz6mYcZ5xxBlu2bGHNmjVccMEF4/bfuXMn9957L0uXLn257cUXX+xkiZJ0UAZ9DYsXL+aqq65i3bp1bN++/aB9X3rpJWbNmsX9998/SdVJh6d2Xbg+HC5aO3VTw/ve9z4+9rGP8cY3vnHcvieeeCL9/f3cfPPNQOtTsA888ECnS5SkA5o2I/omn3V7e3tZuXJl7f433ngjH/jAB/jkJz/J7t27WbZsGWeeeWYHK5SkA5s2Qd+EnTt37tc2f/585s+fD8All1zCJZdcAsDVV1/9cp/+/n7uuOOOSahQksbn1I0kFc6gl6TCTemgz8ymS+iYkh+bpKllygb9zJkz2b59e5GBuPd+9DNnzmy6FEmHgSl7Mba3t5fh4WFGRkaaLqUj9n7DlCR12pQN+hkzZvjtS5LUBlN26kaS1B4GvSQVzqCXpMIZ9JJUOINekgpXK+gj4q8j4uGIeCgi1kTEzIjoj4j1EfFYRHwlIo6q+h5drW+utvd18gFIkg5u3KCPiB7gCmAgM38fOBJYBnwKuC4z5wLPApdWu1wKPJuZrwGuq/pJkhpSd+qmCzgmIrqAY4GngLcCt1TbbwAuqpaXVOtU2xdERLSnXEnSRI0b9Jn5E+Ba4ElaAb8D2AA8l5l7qm7DQE+13ANsrfbdU/Wfve9xI2JFRAxFxFCpn36VpKmgztTNybRG6f3AacBxwKIxuu69Kc1Yo/f9bliTmasycyAzB7q7u+tXLEmakDpTN28DfpyZI5m5G/ga8BZgVjWVA9ALbKuWh4HTAartJwE/b2vVkqTa6tzr5kng3Ig4FngBWAAMAd8G3g7cBCwHbq36r63W/7vafneWeAtKSdpHu76wHNr79al15ujX07qoeh/wYLXPKuDDwJURsZnWHPzqapfVwOyq/UpgsG3VSpImrNbdKzPz48DH92l+AjhnjL67gKWHXpokqR38ZKwkFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVLiupgtQ+/UN3t6W42y55sK2HEdSsxzRS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4WoFfUTMiohbIuLRiNgUEX8QEa+IiDsj4rHq58lV34iIz0TE5ojYGBFndfYhSJIOpu6I/tPAHZn5euBMYBMwCNyVmXOBu6p1gEXA3OrfCuCzba1YkjQh4wZ9RJwI/BGwGiAzf5WZzwFLgBuqbjcAF1XLS4AvZct3gVkRMaftlUuSaqkzon81MAJ8ISJ+EBGfi4jjgFMz8ymA6ucrq/49wNZR+w9Xbb8lIlZExFBEDI2MjBzSg5AkHVidoO8CzgI+m5lvBn7Jb6ZpxhJjtOV+DZmrMnMgMwe6u7trFStJmrg6QT8MDGfm+mr9FlrB/8zeKZnq509H9T991P69wLb2lCtJmqhxgz4znwa2RsTrqqYFwCPAWmB51bYcuLVaXgu8p3r3zbnAjr1TPJKkyVf3i0f+CrgxIo4CngDeS+tJ4qsRcSnwJLC06vsN4AJgM/B81VeS1JBaQZ+Z9wMDY2xaMEbfBC47xLokSW3iJ2MlqXAGvSQVbtp8OXi7vvAa/NJrSYcXR/SSVDiDXpIKZ9BLUuGmzRz9VNWuawdeN5DUKY7oJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgrX1XQBUlP6Bm9v27G2XHNh244ltZsjekkqnEEvSYUz6CWpcAa9JBXOi7GSxtWuC9detG6GI3pJKpxBL0mFM+glqXC1gz4ijoyIH0TEbdV6f0Ssj4jHIuIrEXFU1X50tb652t7XmdIlSXVMZES/Etg0av1TwHWZORd4Fri0ar8UeDYzXwNcV/WTJDWkVtBHRC9wIfC5aj2AtwK3VF1uAC6qlpdU61TbF1T9JUkNqDui/xfgb4GXqvXZwHOZuadaHwZ6quUeYCtAtX1H1f+3RMSKiBiKiKGRkZH/Z/mSpPGMG/QR8efATzNzw+jmMbpmjW2/achclZkDmTnQ3d1dq1hJ0sTV+cDUecDiiLgAmAmcSGuEPysiuqpRey+wreo/DJwODEdEF3AS8PO2Vy5JqmXcEX1m/l1m9mZmH7AMuDsz3wl8G3h71W05cGu1vLZap9p+d2buN6KXJE2OQ3kf/YeBKyNiM605+NVV+2pgdtV+JTB4aCVKkg7FhO51k5nrgHXV8hPAOWP02QUsbUNtkqQ28JOxklQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMJ1NV2ADg99g7e37VhbrrmwbceSDgeO6CWpcAa9JBXOoJekwhn0klS4cYM+Ik6PiG9HxKaIeDgiVlbtr4iIOyPisernyVV7RMRnImJzRGyMiLM6/SAkSQdWZ0S/B/ibzPxd4Fzgsoh4AzAI3JWZc4G7qnWARcDc6t8K4LNtr1qSVNu4QZ+ZT2XmfdXy/wKbgB5gCXBD1e0G4KJqeQnwpWz5LjArIua0vXJJUi0TmqOPiD7gzcB64NTMfApaTwbAK6tuPcDWUbsNV237HmtFRAxFxNDIyMjEK5ck1VI76CPieODfgQ9l5i8O1nWMttyvIXNVZg5k5kB3d3fdMiRJE1Qr6CNiBq2QvzEzv1Y1P7N3Sqb6+dOqfRg4fdTuvcC29pQrSZqoOu+6CWA1sCkz/3nUprXA8mp5OXDrqPb3VO++ORfYsXeKR5I0+erc6+Y84N3AgxFxf9X2EeAa4KsRcSnwJLC02vYN4AJgM/A88N62VixJmpBxgz4z72HseXeABWP0T+CyQ6xLktQmfjJWkgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcB0J+ohYGBE/jIjNETHYiXNIkuppe9BHxJHAvwKLgDcAF0fEG9p9HklSPZ0Y0Z8DbM7MJzLzV8BNwJIOnEeSVENkZnsPGPF2YGFm/mW1/m5gXmZevk+/FcCKavV1wA/bVMIpwM/adKx2saZ6rKm+qViXNdXTzppelZnd43XqatPJRosx2vZ7NsnMVcCqtp88YigzB9p93ENhTfVYU31TsS5rqqeJmjoxdTMMnD5qvRfY1oHzSJJq6ETQfx+YGxH9EXEUsAxY24HzSJJqaPvUTWbuiYjLgW8CRwKfz8yH232eg2j7dFAbWFM91lTfVKzLmuqZ9JrafjFWkjS1+MlYSSqcQS9JhTPoJalwnXgf/aSJiNfT+tRtD6336m8D1mbmpkYLUy0RcQ6Qmfn96jYZC4FHM/MbDZf2soj4Uma+p+k6ND2Neufhtsz8z4h4B/AWYBOwKjN3T0od0/VibER8GLiY1i0WhqvmXlr/UW/KzGuaqm0qqp4Ue4D1mblzVPvCzLyjgXo+Tut+SF3AncA8YB3wNuCbmfmPDdS079uAA/hj4G6AzFw82TXtKyL+kNZtRh7KzG81WMc8YFNm/iIijgEGgbOAR4B/yswdDdR0BfD1zNw62ec+kIi4kdbf+LHAc8DxwNeABbTyd/mk1DGNg/5HwO/t+4xYPYM+nJlzm6nswCLivZn5hQbOewVwGa1RxJuAlZl5a7Xtvsw8q4GaHqxqORp4GugdFRrrM/OMBmq6j1ZQfY7WK8QA1tAaPJCZ32mgpu9l5jnV8vtp/R6/Dvwp8B9NDWgi4mHgzOrt1KuA54FbaAXYmZn5Fw3UtAP4JfA4rd/bzZk5Mtl17FPTxsw8IyK6gJ8Ap2XmryMigAcm6+98Os/RvwScNkb7nGrbVPSJhs77fuDszLwImA/8fUSsrLaNdcuKybAnM3+dmc8Dj2fmLwAy8wWa+/0NABuAjwI7MnMd8EJmfqeJkK/MGLW8AviTzPwEraB/ZzMlAXBEZu6plgcy80OZeU9V26sbqukJWq/q/wE4G3gkIu6IiOURcUJDNR1RDT5PoDWqP6lqP5rf/t121HSeo/8QcFdEPAbsfan2O8BrgMsPuFeHRcTGA20CTp3MWkY5cu90TWZuiYj5wC0R8SqaC/pfRcSxVdCfvbcxIk6ioaDPzJeA6yLi5urnMzT//8gREXEyrUFZ7B2hZuYvI2LPwXftqIdGvUJ9ICIGMnMoIl4LTMq88xiy+h1+C/hWRMygNT14MXAtMO7NvzpgNfAorQ+PfhS4OSKeAM6lNe08Kabt1A1ARBxBa76yh1ZgDQPfz8xfN1jTM8CfAc/uuwm4NzPHehXS6ZruBq7MzPtHtXUBnwfemZlHNlDT0Zn54hjtpwBzMvPBya5pjFouBM7LzI80WMMWWk98QWs66S2Z+XREHA/ck5lvaqiuk4BPA+fTuhPjWbQGXFuBKzLzgQZq+kFmvvkA246pXi1Ouog4DSAzt0XELFrXoZ7MzO9NWg3TOeinoohYDXwhM+8ZY9u/ZeY7Gqipl9ZUydNjbDsvM/9rsmvSoYmIY4FTM/PHDddxAq2pmi5gODOfabCW12bmj5o6/1Rm0EtS4abzxVhJUg0GvSQVzqCXpMIZ9JJUuP8DYhuh8ZtNJn0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot(kind = 'bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
