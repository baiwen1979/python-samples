{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 基于Hadoop Streaming的Python大数据分析\n",
    "---\n",
    "## 1. 概述\n",
    "\n",
    "Hadoop Streaming是Hadoop提供的一种编程工具，允许用户用任何可执行程序和脚本作为**Mapper**和**Reducer**来完成Map/Reduce任务，这意味完全用户可以用Hadoop Streaming+Python/Ruby/Golang/C++ 等任何熟悉的语言来完成大数据处理和分析的任务。\n",
    "\n",
    "## 2. Hadoop Streaming的工作方式\n",
    "\n",
    "**Hadoop Streaming**的工作方式如下图所示。与标准的MapReduce(以下简称MR)一样，整个MR过程依然由**Mapper**、\\[Combiner\\]、**Reducer**组成(其中Combiner为可选加入)。用户可以像使用Java一样去用其他语言编写MR，只不过Mapper/Reducer的输入和输出并与Java API打交道，而是通过该语言下的标准输入输出函数来进行。图中标注了用户编写的Mapper和Reducer的位置。\n",
    "![Hadoop Streaming的工作方式](images/hadoopstream.jpg)\n",
    "\n",
    "### Mapper的角色\n",
    "\n",
    "Hadoop将用户提交的**Mapper**可执行程序或脚本作为一个单独的进程加载起来，这个进程我们称之为mapper进程，hadoop不断地将文件片段转换为行，传递到我们的**Mapper**进程中，**Mapper**进程通过**标准输入**(STDIN) 的方式一行一行地获取这些数据，然后设法将其转换为**键值对**(Key/Value)，再通过**标准输出**(STDOUT)的形式将这些**键值对**按照**一对一行**的方式输出。\n",
    "\n",
    "虽然在**Mapper**程序中，我们自己能分得清**Key/Value**(比如：string key,int value)，但是当我们采用标准输出之后，Key/Value是打印到一行作为结果输出的(比如`print(\"{0}\\{1}\".format(key,value))`，因此,为了保证Hadoop能从中鉴别出键和值，**键值对**一定要以分隔符'\\t'即Tab(也可自定义分隔符)字符分隔，这样才能保证hadoop正确地进行分区（Partition）、重排（shuffle）等过程。\n",
    "\n",
    "### Reducer的角色\n",
    "\n",
    "Hadoop将用户提交的**Reducer**可执行程序或脚本同样作为一个单独的进程加载起来，这个进程被称为**Reducer**进程，hadoop不断地将**键值对**(按键排序)按照**一对儿一行**的方式传递到**Reducer**进程中，Reducer进程同样通过标准输入的方式按行获取这些**键值对**，进行**自定义计算**后将结果通过标准输出的形式输出。\n",
    "\n",
    "在**Reducer**这个过程中需要注意的是：传递到**Reducer**的键值对是按照键排过序的，这一点是由MR框架的**Sort**过程保证的，因此如果读到一个键与前一个键不同，我们就可以知道当前Key对应的**键值对**已经结束了，接下来将是新的Key对应的**键值对**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hadoop Streaming的使用方式\n",
    "\n",
    "Hadoop Streaming的使用方式形如：\n",
    "```\n",
    "$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/.../hadoop-streaming.jar [genericOptions] [streamingOptions]\n",
    "```\n",
    "在这行命令中，其实是有先后顺序的，一定要保证 `[genericOptions]`写在`[streamingOptions]`之前，否则Hadoop Streaming命令将失效。\n",
    "\n",
    "按照以上的格式使用该命令需要在安装hadoop时设置好环境变量HADOOP_HOME，将其值设置为hadoop的安装目录，当然如果觉得以上的调用方式还是麻烦的话，也可以把hadoop设置进系统的PATH环境变量并用`hadoop jar $HADOOP_HOME/.../hadoop-streaming.jar [genericOptions] [streamingOptions]`的格式调用。\n",
    "\n",
    "另外在指定hadoop-streaming.jar时，可能由于装的hadoop版本不同，那这个jar的位置也不同，需要根据自己的实际情况来确定这个hadoop-streaming.jar的位置，并将其填入命令中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### genericOptions ###\n",
    "\n",
    "常用的genericOptions如下：\n",
    "\n",
    "* **-D property=value**: 指定额外的配置信息变量，详情在后文介绍\n",
    "* **-files file1,file2,...**: 指定需要拷贝到集群节点的文件，以逗号分隔，通常为自己编写的Mapper和Reducer脚本或可执行程序。Mapper和Reducer通常要由集群中不同的节点来执行，而很脚本或可执行程序可能仅存在于提交任务时所用的那个节点上，因此需要将其分发出去。\n",
    "\n",
    "### streamingOptions ###\n",
    "\n",
    "常用的streamingOptions如下：\n",
    "\n",
    "* **-file filename**: 指定需要拷贝到集群节点的文件，与-files的功能类似，只不过如果使用-file的话，就需要一个文件一个文件地去上传，比方说如果我要将我的mapper.py，reducer.py上传到集群上去运行，那就得需要两个-file参数。在实际使用-file时，hadoop似乎并不希望我们使用-file参数，会出现如下警告：\n",
    "```\n",
    "warning。“18/03/26 20:17:40 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
    "```\n",
    "* **-input myInputDirs**: 指定给MapReduce任务输入的文件位置，通常为hdfs上的文件路径，多个文件或目录用逗号隔开\n",
    "* **-output myOutputDir**: 指定给mapreduce任务输出的目录，通常为hdfs上的文件路径\n",
    "* **-mapper**: executable or JavaClassName，用于Mapper的可执行程序或Java类，如果是脚本文件，应该以命令行完整调用的格式作为可执行程序参数并且须加引号，比如`-mapper \"python mapper.py\"`\n",
    "* **-reducer**: executable or JavaClassName，用于Reducer的可执行程序或Java类，要求同上\n",
    "* **-partitioner**: JavaClassName,自定义的partitioner Java类\n",
    "* **-combiner**: streamingCommandor JavaClassName,自定义的combiner类或命令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -D property=value\n",
    "\n",
    "常用的-D property=value如下：\n",
    "\n",
    "* **-D mapred.job.name=jobname**: 指定作业名称\n",
    "* **-D mapred.map.tasks=numofmap**: 每个Job运行map task的数量\n",
    "* **-D mapred.reduce.tasks=numofreduce**: 每个Job运行reduce task的数量，如果指定为0，则意味着提交了一个map only的任务\n",
    "* **-D stream.map.input.field.separator**: 指定map输入时的分隔符，默认为\"\\t\"\n",
    "* **-D stream.map.output.field.separator**: 指定map输出时使用的Key/Value分隔符，默认为\"\\t\"，比如在我们的mapper中，输出Key/Value pairs的标准输出语句很可能是这样的 print(\"{0},{1}\".format(k,v))，由于使用了非默认的分隔符，因此需要额外指定分隔符\",\"\n",
    "* **-D stream.reduce.input.field.separator**: 指定reduce输入时的分隔符，默认为\"\\t\"\n",
    "* **-D stream.reduce.output.field.separator**: 指定reduce输出时的分隔符，默认为\"\\t\"\n",
    "* **-D stream.num.map.output.key.fields=num**: 指定map输出中第几个分隔符作为key和value的分隔点，默认为1\n",
    "* **-D stream.num.reduce.output.fields=num**: 指定reduce输出中第几个分隔符作为key和value的分隔点，默认为1\n",
    "* **-D stream.non.zero.exit.is.failure=false/true**: 指定当mapper和reducer未返回0时，hadoop是否该认为此任务执行失败。默认为true。当mapper和reducer的返回值不是0或没有返回值时，hadoop将认为该任务为异常任务，将被再次执行，默认尝试4次都不是0，整个job都将失败。因此，如果我们在编写mapper和reducer未返回0时，则应该将该参数设置为false，否则hadoop streaming任务将报出异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 示例数据与程序\n",
    "\n",
    "### 示例数据\n",
    "\n",
    "该数据为部分儿童的样本数据sample_data.csv，每行由user_id（用户ID）,birthday（生日），gender（性别，0为女，1为男，2为未知）组成。假设我们的问题是：在这些儿童中，每一年出生的男孩和女孩各是多少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,birthday,gender\n",
      "27571234,20130311,1\n",
      "41597123,20121111,0\n",
      "13725721,20120130,1\n",
      "10339332,20110910,0\n",
      "10642245,20130213,0\n",
      "10923201,20110830,1\n",
      "11768880,20120107,1\n",
      "12519465,20130705,1\n",
      "12950574,20090708,0\n",
      "13735440,20120323,0\n",
      "14510892,20140812,1\n",
      "14905422,20110429,1\n",
      "15786531,20080922,0\n",
      "16265490,20091209,0\n",
      "17431245,20110115,0\n",
      "18190851,20110101,0\n",
      "20087991,20100808,0\n",
      "20570454,20081017,1\n",
      "21137271,20110204,1\n",
      "21415917,20060801,1\n",
      "21887268,20100526,0\n",
      "22602471,20090601,1\n",
      "23208537,20080416,1\n",
      "23927133,20081029,0\n",
      "24829944,20140826,1\n",
      "52529655,20130611,2"
     ]
    }
   ],
   "source": [
    "!cat data/sample_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流程解析\n",
    "\n",
    "首先梳理下使用此数据的MapReduce场景，并思考**Mapper**接收到的数据是什么样的，又应该将处理后的数据输出成什么样的。流程如下图（有缩减）:\n",
    "\n",
    "![解析流程](images/mrflow.jpg)\n",
    "\n",
    "#### 编写Mapper的思路\n",
    "一行一行的获取MR传入的原始数据记录，然后将记录分割成多个字段，获取其中的生年和性别字段，之后将结果打印到标准输出中。需要注意的是该段数据是有header的,要想办法跳过这段header。\n",
    "\n",
    "#### 编写Reducer的思路\n",
    "一行一行地获取到按key排过序的Key/Value对(“排序行为”是MR框架为做的，不需要我们自己指定)，由于MR框架已经为我们排好序，因此只要观察到当前行获得的key与上一行获得的key不一样，即可判断是新的生年/性别组，然后累加每一组的男孩和女孩数，遇到新的组时将上一生年/性别组的男孩和女孩数目打印出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例程序\n",
    "\n",
    "#### Mapper（sample_mapper.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    row = line.split(',')\n",
    "    user_id = row[0]\n",
    "    if user_id == \"user_id\":\n",
    "        continue\n",
    "    birth_year = row[1][0:4]\n",
    "    gender = row[2]\n",
    "    print(\"{0}\\t{0}\"format(birth_year,gender))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer（sample_reducer.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "num_by_gender = {'0': 0, '1': 0, '2': 0}\n",
    "last_key = False\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()    \n",
    "    row = line.split('\\t')\n",
    "    cur_key = row[0]\n",
    "    gender = row[1]\n",
    "    if last_key and cur_key != last_key:\n",
    "        print(\"Y:{0},F:{1},M:{2}\".\n",
    "              format(last_key,num_by_gender['0'],num_by_gender['1']))\n",
    "        last_key = cur_key\n",
    "        num_by_gender = {'0': 0, '1': 0, '2': 0}\n",
    "        num_by_gender[gender] += 1\n",
    "    else:\n",
    "        last_key = cur_key\n",
    "        num_by_gender[gender] += 1\n",
    "        \n",
    "if last_key:\n",
    "    print(\"Y:{0},F:{1},M:{2}\".\n",
    "          format(last_key,num_by_gender['0'],num_by_gender['1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本地测试\n",
    "\n",
    "在提交MapReduce作业之前，可以使用Linux的管道操作来测试一下编写的Mapper和Reducer。使用Linux的管道，可以轻易地连接两个毫不相关的程序，把一个程序的结果交给另一个来处理，甚至，不停地交接处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:2006,F:0,M:1\n",
      "Y:2008,F:2,M:2\n",
      "Y:2009,F:2,M:1\n",
      "Y:2010,F:2,M:0\n",
      "Y:2011,F:3,M:3\n",
      "Y:2012,F:2,M:2\n",
      "Y:2013,F:1,M:2\n",
      "Y:2014,F:0,M:2\n"
     ]
    }
   ],
   "source": [
    "!cat data/sample_data.csv | \\\n",
    "python input_files/sample_mapper.py | \\\n",
    "sort -t '\\t' -k 1 | \\\n",
    "python input_files/sample_reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在集群上运行MapReduce程序\n",
    "\n",
    "接着把sample_data.csv上传到hdfs上，并使用Hadoop Streaming命令来在集群上运行我们的程序。请注意一定要将-D，-files参数放在所有参数前面，因为genericOptions一定要放在streamingOptions前面，而-files -D都属于genericOptions。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 操作代码\n",
    "!docker cp ./data/sample_data.csv master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master \\\n",
    "hadoop fs -put /root/sample_data.csv /input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp ./input_files/sample_mapper.py master:/root/\n",
    "!docker cp ./input_files/sample_reducer.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/14 12:59:58 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
      "Deleted /output/sample_data\n",
      "19/08/14 13:00:00 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/root/sample_mapper.py, /root/sample_reducer.py] [] /tmp/streamjob147488410626420435.jar tmpDir=null\n",
      "19/08/14 13:00:01 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/08/14 13:00:01 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/08/14 13:00:01 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "19/08/14 13:00:02 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/08/14 13:00:02 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/08/14 13:00:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "19/08/14 13:00:02 INFO Configuration.deprecation: mapred.job.name is deprecated. Instead, use mapreduce.job.name\n",
      "19/08/14 13:00:02 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/08/14 13:00:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local114411754_0001\n",
      "19/08/14 13:00:03 INFO mapred.LocalDistributedCacheManager: Localized file:/root/sample_mapper.py as file:/tmp/hadoop-root/mapred/local/1565787602686/sample_mapper.py\n",
      "19/08/14 13:00:03 INFO mapred.LocalDistributedCacheManager: Localized file:/root/sample_reducer.py as file:/tmp/hadoop-root/mapred/local/1565787602687/sample_reducer.py\n",
      "19/08/14 13:00:03 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/08/14 13:00:03 INFO mapreduce.Job: Running job: job_local114411754_0001\n",
      "19/08/14 13:00:03 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/08/14 13:00:03 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/08/14 13:00:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/14 13:00:03 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/14 13:00:03 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/08/14 13:00:03 INFO mapred.LocalJobRunner: Starting task: attempt_local114411754_0001_m_000000_0\n",
      "19/08/14 13:00:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/14 13:00:03 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/14 13:00:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: Processing split: file:/input/sample_data.csv:0+543\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/08/14 13:00:03 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./sample_mapper.py]\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "19/08/14 13:00:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "19/08/14 13:00:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/14 13:00:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/14 13:00:03 INFO streaming.PipeMapRed: Records R/W=27/1\n",
      "19/08/14 13:00:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/14 13:00:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/14 13:00:03 INFO mapred.LocalJobRunner: \n",
      "19/08/14 13:00:03 INFO mapred.MapTask: Starting flush of map output\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: Spilling map output\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: bufstart = 0; bufend = 182; bufvoid = 104857600\n",
      "19/08/14 13:00:03 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214296(104857184); length = 101/6553600\n",
      "19/08/14 13:00:04 INFO mapred.MapTask: Finished spill 0\n",
      "19/08/14 13:00:04 INFO mapred.Task: Task:attempt_local114411754_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: Records R/W=27/1\n",
      "19/08/14 13:00:04 INFO mapred.Task: Task 'attempt_local114411754_0001_m_000000_0' done.\n",
      "19/08/14 13:00:04 INFO mapred.Task: Final Counters for attempt_local114411754_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2408\n",
      "\t\tFILE: Number of bytes written=375826\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=26\n",
      "\t\tMap output bytes=182\n",
      "\t\tMap output materialized bytes=240\n",
      "\t\tInput split bytes=79\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=26\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=201326592\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=567\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local114411754_0001_m_000000_0\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: Starting task: attempt_local114411754_0001_r_000000_0\n",
      "19/08/14 13:00:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/14 13:00:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/14 13:00:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/14 13:00:04 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1c78ecf4\n",
      "19/08/14 13:00:04 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/14 13:00:04 INFO reduce.EventFetcher: attempt_local114411754_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/08/14 13:00:04 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local114411754_0001_m_000000_0 decomp: 236 len: 240 to MEMORY\n",
      "19/08/14 13:00:04 INFO reduce.InMemoryMapOutput: Read 236 bytes from map-output for attempt_local114411754_0001_m_000000_0\n",
      "19/08/14 13:00:04 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 236, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->236\n",
      "19/08/14 13:00:04 WARN io.ReadaheadPool: Failed readahead on ifile\n",
      "EBADF: Bad file descriptor\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)\n",
      "\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "19/08/14 13:00:04 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/14 13:00:04 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/14 13:00:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/14 13:00:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 229 bytes\n",
      "19/08/14 13:00:04 INFO reduce.MergeManagerImpl: Merged 1 segments, 236 bytes to disk to satisfy reduce memory limit\n",
      "19/08/14 13:00:04 INFO reduce.MergeManagerImpl: Merging 1 files, 240 bytes from disk\n",
      "19/08/14 13:00:04 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/14 13:00:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/14 13:00:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 229 bytes\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/14 13:00:04 INFO mapreduce.Job: Job job_local114411754_0001 running in uber mode : false\n",
      "19/08/14 13:00:04 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/spark-2.3.0/./sample_reducer.py]\n",
      "19/08/14 13:00:04 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/08/14 13:00:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/08/14 13:00:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/08/14 13:00:04 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/14 13:00:04 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/14 13:00:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/14 13:00:04 INFO streaming.PipeMapRed: Records R/W=26/1\n",
      "19/08/14 13:00:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/14 13:00:04 INFO mapred.Task: Task:attempt_local114411754_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/14 13:00:04 INFO mapred.Task: Task attempt_local114411754_0001_r_000000_0 is allowed to commit now\n",
      "19/08/14 13:00:04 INFO output.FileOutputCommitter: Saved output of task 'attempt_local114411754_0001_r_000000_0' to file:/output/sample_data/_temporary/0/task_local114411754_0001_r_000000\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: Records R/W=26/1 > reduce\n",
      "19/08/14 13:00:04 INFO mapred.Task: Task 'attempt_local114411754_0001_r_000000_0' done.\n",
      "19/08/14 13:00:04 INFO mapred.Task: Final Counters for attempt_local114411754_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2920\n",
      "\t\tFILE: Number of bytes written=376206\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=240\n",
      "\t\tReduce input records=26\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=26\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=20\n",
      "\t\tTotal committed heap usage (bytes)=201850880\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=140\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local114411754_0001_r_000000_0\n",
      "19/08/14 13:00:04 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/08/14 13:00:05 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/08/14 13:00:05 INFO mapreduce.Job: Job job_local114411754_0001 completed successfully\n",
      "19/08/14 13:00:05 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5328\n",
      "\t\tFILE: Number of bytes written=752032\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=27\n",
      "\t\tMap output records=26\n",
      "\t\tMap output bytes=182\n",
      "\t\tMap output materialized bytes=240\n",
      "\t\tInput split bytes=79\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=240\n",
      "\t\tReduce input records=26\n",
      "\t\tReduce output records=8\n",
      "\t\tSpilled Records=52\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=20\n",
      "\t\tTotal committed heap usage (bytes)=403177472\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=567\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=140\n",
      "19/08/14 13:00:05 INFO streaming.StreamJob: Output directory: /output/sample_data\n",
      "Found 2 items\n",
      "-rw-r--r--   1 root root          0 2019-08-14 13:00 /output/sample_data/_SUCCESS\n",
      "-rw-r--r--   1 root root        128 2019-08-14 13:00 /output/sample_data/part-00000\n"
     ]
    }
   ],
   "source": [
    "!docker exec master \\\n",
    "hadoop fs -rm -f -r /output/sample_data\n",
    "!docker exec master \\\n",
    "hadoop jar /usr/hadoop-2.8.3/share/hadoop/tools/lib/hadoop-streaming-2.8.3.jar \\\n",
    "-D stream.num.map.output.key.fields=1 \\\n",
    "-D num.key.fields.for.partition=1 \\\n",
    "-D mapred.map.tasks=10 \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D mapred.job.name=\"Sample Data\" \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-input /input/sample_data.csv \\\n",
    "-output /output/sample_data \\\n",
    "-mapper /root/sample_mapper.py \\\n",
    "-reducer /root/sample_reducer.py \\\n",
    "-file /root/sample_mapper.py \\\n",
    "-file /root/sample_reducer.py\n",
    "!docker exec master \\\n",
    "hadoop fs -ls /output/sample_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:2006,F:0,M:1\t\n",
      "Y:2008,F:2,M:2\t\n",
      "Y:2009,F:2,M:1\t\n",
      "Y:2010,F:2,M:0\t\n",
      "Y:2011,F:3,M:3\t\n",
      "Y:2012,F:2,M:2\t\n",
      "Y:2013,F:1,M:2\t\n",
      "Y:2014,F:0,M:2\t\n"
     ]
    }
   ],
   "source": [
    "!docker exec master \\\n",
    "hadoop fs -cat /output/sample_data/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在集群上运行的结果和预期一致，并与管道测试结果一致。当然也有可能出现不一致的情况，如果是这样我们该关注的点应该放到分布式架构上面而不是代码的计算逻辑上，因为在集群上运行和本机测试主要的不同是：在集群上是多个Mapper和Reducer共同运行，而在本机你可以理解为一个Mapper和Reducer，这种不同可能会导致不同的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 注意事项\n",
    "\n",
    "### Hadoop Streaming与python环境\n",
    "\n",
    "hadoop是基于集群的，因此MR任务是运行于集群中的各个节点上的，正如使用集群时需要为集群中的节点安装Java环境一样，如果你想用Python来实现MapReduce，当然也需要为各个节点配置好python环境。\n",
    "\n",
    "那么问题就来了，我应该为节点配置什么样的环境，如何让hadoop命令或者python脚本与该环境一致。这里将涉及三个方面:\n",
    "\n",
    "#### 需要保证python安装正确并且能使用python命令行的方式启动python\n",
    "  \n",
    "-files参数表示将mapper.py和reducer.py发送到集群中的各个节点去运行，-mapper \"python mapper.py\"表示发送到这些节点的.py文件是使用\"python xxx.py\"的命令行方式启动成进程的。因此必须保证这些节点拥有这样的能力。检验的方法是在节点的Terminal下面输入python -V，如果能正确显示python版本则表示安装正确，否则可能需要花时间把python重新安装好。\n",
    "\n",
    "#### 需要保证python命令启动的是节点中的哪个版本\n",
    "\n",
    "如果一定要使用python3作为MR，那么要在/usr/bin下面建立python到python3安装目录下bin/python3的软连接。\n",
    "\n",
    "#### 需要保证代码语法适应所有节点安装的python版本\n",
    "\n",
    "如果集群上安装的是python2的环境，而Mapper和Reducer使用的是python3的语法编写的，那一定跑不起来，会报一堆难以排查的错误\n",
    "\n",
    "### 打包python环境到集群\n",
    "\n",
    "有一个办法可以解决为各个节点配置环境的问题，那就是将环境打包到集群，并在执行命令时分发到各个节点供其使用。为此，首先将python安装目录压缩，然后上传到hdfs，之后在hadoop streaming命令中通过设置`-archives \"hdfs://master/env/python35.tar.gz#py`参数即可把这个hdfs上的Python环境分发到集群上各个节点上去运行。其中的#py表示将hdfs上的这个文件分发到集群中各个节点之后再解压到名为py的文件夹中，因此需要在-mapper中使用解压后文件夹中的python程序来启动脚本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 附录\n",
    "\n",
    "### Hadoop Streaming的官方Jar包注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "JAR_PACKAGE = '/usr/hadoop-2.8.3/share/\\\n",
    "hadoop/tools/lib/hadoop-streaming-2.8.3.jar'\n",
    "!docker exec master hadoop jar $JAR_PACKAGE -info > hadoopStreaming_doc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar [options]\n",
      "Options:\n",
      "  -input          <path> DFS input file(s) for the Map step.\n",
      "  -output         <path> DFS output directory for the Reduce step.\n",
      "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
      "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
      "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
      "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
      "                  Deprecated. Use generic option \"-files\" instead.\n",
      "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
      "                  Optional. The input format class.\n",
      "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
      "                  Optional. The output format class.\n",
      "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
      "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
      "  -inputreader    <spec> Optional. Input recordreader spec.\n",
      "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
      "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
      "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
      "  -io             <identifier> Optional. Format to use for input to and output\n",
      "                  from mapper/reducer commands\n",
      "  -lazyOutput     Optional. Lazily create Output.\n",
      "  -background     Optional. Submit the job and don't wait till it completes.\n",
      "  -verbose        Optional. Print verbose output.\n",
      "  -info           Optional. Print detailed usage.\n",
      "  -help           Optional. Print help message.\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "command [genericOptions] [commandOptions]\n",
      "\n",
      "\n",
      "Usage tips:\n",
      "In -input: globbing on <path> is supported and can have multiple -input\n",
      "\n",
      "Default Map input format: a line is a record in UTF-8 the key part ends at first\n",
      "  TAB, the rest of the line is the value\n",
      "\n",
      "To pass a Custom input format:\n",
      "  -inputformat package.MyInputFormat\n",
      "\n",
      "Similarly, to pass a custom output format:\n",
      "  -outputformat package.MyOutputFormat\n",
      "\n",
      "The files with extensions .class and .jar/.zip, specified for the -file\n",
      "  argument[s], end up in \"classes\" and \"lib\" directories respectively inside\n",
      "  the working directory when the mapper and reducer are run. All other files\n",
      "  specified for the -file argument[s] end up in the working directory when the\n",
      "  mapper and reducer are run. The location of this working directory is\n",
      "  unspecified.\n",
      "\n",
      "To set the number of reduce tasks (num. of output files) as, say 10:\n",
      "  Use -numReduceTasks 10\n",
      "To skip the sort/combine/shuffle/sort/reduce step:\n",
      "  Use -numReduceTasks 0\n",
      "  Map output then becomes a 'side-effect output' rather than a reduce input.\n",
      "  This speeds up processing. This also feels more like \"in-place\" processing\n",
      "  because the input filename and the map input order are preserved.\n",
      "  This is equivalent to -reducer NONE\n",
      "\n",
      "To speed up the last maps:\n",
      "  -D mapreduce.map.speculative=true\n",
      "To speed up the last reduces:\n",
      "  -D mapreduce.reduce.speculative=true\n",
      "To name the job (appears in the JobTracker Web UI):\n",
      "  -D mapreduce.job.name='My Job'\n",
      "To change the local temp directory:\n",
      "  -D dfs.data.dir=/tmp/dfs\n",
      "  -D stream.tmpdir=/tmp/streaming\n",
      "Additional local temp directories with -jt local:\n",
      "  -D mapreduce.cluster.local.dir=/tmp/local\n",
      "  -D mapreduce.jobtracker.system.dir=/tmp/system\n",
      "  -D mapreduce.cluster.temp.dir=/tmp/temp\n",
      "To treat tasks with non-zero exit status as SUCCEDED:\n",
      "  -D stream.non.zero.exit.is.failure=false\n",
      "Use a custom hadoop streaming build along with standard hadoop install:\n",
      "  $HADOOP_PREFIX/bin/hadoop jar /path/my-hadoop-streaming.jar [...]\\\n",
      "    [...] -D stream.shipped.hadoopstreaming=/path/my-hadoop-streaming.jar\n",
      "For more details about jobconf parameters see:\n",
      "  http://wiki.apache.org/hadoop/JobConfFile\n",
      "Truncate the values of the job configuration copiedto the environment at the given length:\n",
      "   -D stream.jobconf.truncate.limit=-1\n",
      "To set an environment variable in a streaming command:\n",
      "   -cmdenv EXAMPLE_DIR=/home/example/dictionaries/\n",
      "\n",
      "Shortcut:\n",
      "   setenv HSTREAMING \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n",
      "\n",
      "Example: $HSTREAMING -mapper \"/usr/local/bin/perl5 filter.pl\"\n",
      "           -file /local/filter.pl -input \"/logs/0604*/*\" [...]\n",
      "  Ships a script, invokes the non-shipped perl interpreter. Shipped files go to\n",
      "  the working directory so filter.pl is found by perl. Input files are all the\n",
      "  daily logs for days in month 2006-04\n"
     ]
    }
   ],
   "source": [
    "!cat hadoopStreaming_doc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
