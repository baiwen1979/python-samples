{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 项目实战 - IBM股票价格数据离线分析\n",
    "---\n",
    "\n",
    "**实战内容**：\n",
    "\n",
    "本节以IBM股票价格数据离线分析为例，学习使用Hadoop大数据平台和Python、pandas等工具进行数据分析的整个方法和流程。\n",
    "\n",
    "## 分析目标\n",
    "\n",
    "### IBM股票价格数据\n",
    "\n",
    "#### Yahoo财经IBM股票价格数据来源：\n",
    "* 历史股票价格数据：https://finance.yahoo.com/quote/IBM/history?p=IBM\n",
    "* 数据字段信息：日期、开盘价、最高价、最低价、收盘价、调整的收益盘和交易量，使用逗号分隔(Date,Open,High,Low,Close,AdjClose,Volume)\n",
    "\n",
    "#### 数据分析需求：\n",
    "* 计算股票价格变化的日内历史百分比\n",
    "* 计算公式：开盘价和收盘价计算一日价格的变化 = (开盘价 - 收盘价) / 开盘价，统计每日变化百分比的总数\n",
    "\n",
    "#### 可视化展示\n",
    "* 绘制分析分析结果柱状图：读入输出的每一行，将结果分隔为x和y值的列表，然后使用matplotlib的bar函数进行绘图，宽度(width)缩小为0.1\n",
    "\n",
    "## 分析过程\n",
    "\n",
    "### 1. 下载数据\n",
    "\n",
    "使用浏览器或者使用wget命令将IBM股票价格数据下载到本地服务器（或hadoop的namenode，如master）中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-12 20:38:53--  http://www.baidu.com/\n",
      "正在解析主机 www.baidu.com (www.baidu.com)... 220.181.38.150, 220.181.38.149\n",
      "正在连接 www.baidu.com (www.baidu.com)|220.181.38.150|:80... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：2381 (2.3K) [text/html]\n",
      "正在保存至: “index.html”\n",
      "\n",
      "index.html          100%[===================>]   2.33K  --.-KB/s  用时 0.003s    \n",
      "\n",
      "2019-08-12 20:38:53 (888 KB/s) - 已保存 “index.html” [2381/2381])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.baidu.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query1.finance.yahoo.com/v7/finance/download/IBM?\\\n",
    "period1=1534035744&period2=1565571744&\\\n",
    "interval=1d&events=history&crumb=h3f81hxI4MP'\n",
    "user_agent='\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US)\\\n",
    "AppleWebKit/534.16 (KHTML, like Gecko) \\\n",
    "Chrome/10.0.648.204 Safari/534.16\"'\n",
    "!wget -O -v --user-agent $user_agent ./ibm_stork.csv $url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2168\n",
      "-rw-r--r--  1 xiaobai  staff   31435  8 11 16:40 Docker_Hadoop_Cluster.ipynb\n",
      "-rw-r--r--  1 xiaobai  staff     430  8 11 15:20 Dockerfile\n",
      "-rw-r--r--  1 xiaobai  staff  895558  8 12 20:07 ch01.ipynb\n",
      "-rw-r--r--  1 xiaobai  staff   57958  8 12 20:36 ch02.ipynb\n",
      "drwxr-xr-x  4 xiaobai  staff     128  8 11 20:20 \u001b[34mconf\u001b[m\u001b[m\n",
      "drwxr-xr-x  9 xiaobai  staff     288  8 12 20:24 \u001b[34mdata\u001b[m\u001b[m\n",
      "-rw-r--r--@ 1 xiaobai  staff    3435  8 11 20:29 docker-compose.yml\n",
      "drwxr-xr-x  4 xiaobai  staff     128  8 11 18:05 \u001b[34mhadoop\u001b[m\u001b[m\n",
      "-rw-r--r--@ 1 xiaobai  staff    1189  8 11 20:13 hadoop.env\n",
      "-rw-r--r--@ 1 xiaobai  staff    2381  1 23  2017 index.html\n",
      "drwxr-xr-x  2 xiaobai  staff      64  8 11 20:19 \u001b[34minput_files\u001b[m\u001b[m\n",
      "drwxr-xr-x  2 xiaobai  staff      64  8 11 20:19 \u001b[34mjars\u001b[m\u001b[m\n",
      "-rw-r--r--  1 xiaobai  staff   84452  7 31 23:26 pandas-tutorial.ipynb\n",
      "-rwxr-xr-x  1 xiaobai  staff     568  8 12 16:42 \u001b[31mstock_mapper.py\u001b[m\u001b[m\n",
      "-rwxr-xr-x  1 xiaobai  staff     758  8 12 16:43 \u001b[31mstock_reducer.py\u001b[m\u001b[m\n",
      "-rwxr-xr-x  1 xiaobai  staff     603  8 11 17:39 \u001b[31mword_count_mapper.py\u001b[m\u001b[m\n",
      "-rwxr-xr-x  1 xiaobai  staff    1144  8 12 12:08 \u001b[31mword_count_reducer.py\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若无法用wget下载，改用浏览器下载，并保存到./data目录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16064\n",
      "-rw-r--r--@ 1 xiaobai  staff    18558  8 12 09:04 IBM.csv\n",
      "-rw-r--r--@ 1 xiaobai  staff  2377193  8 11 21:42 gone_with_wind.txt\n",
      "-rw-r--r--@ 1 xiaobai  staff   674570  8 11 18:19 pg20417.txt\n",
      "-rw-r--r--@ 1 xiaobai  staff  1586393  8 11 18:20 pg4300.txt\n",
      "-rw-r--r--@ 1 xiaobai  staff  1428841  8 11 18:20 pg5000.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改名\n",
    "!mv data/IBM.csv data/ibm_stock.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16064\n",
      "-rw-r--r--@ 1 xiaobai  staff  2377193  8 11 21:42 gone_with_wind.txt\n",
      "-rw-r--r--@ 1 xiaobai  staff    18558  8 12 09:04 ibm_stock.csv\n",
      "-rw-r--r--@ 1 xiaobai  staff   674570  8 11 18:19 pg20417.txt\n",
      "-rw-r--r--@ 1 xiaobai  staff  1586393  8 11 18:20 pg4300.txt\n",
      "-rw-r--r--@ 1 xiaobai  staff  1428841  8 11 18:20 pg5000.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29,151.110001,151.479996,150.320007,150.880005,149.122940,2075500\n",
      "2019-07-30,150.000000,150.399994,149.220001,149.770004,148.025864,2632700\n",
      "2019-07-31,149.699997,150.179993,146.389999,148.240005,146.513687,3722900\n",
      "2019-08-01,148.899994,152.820007,148.500000,150.320007,148.569458,6344000\n",
      "2019-08-02,149.029999,152.949997,145.589996,147.250000,145.535202,8101700\n",
      "2019-08-05,144.979996,145.100006,139.149994,140.759995,139.120789,7114900\n",
      "2019-08-06,142.029999,142.470001,139.309998,140.729996,139.091141,5070700\n",
      "2019-08-07,138.740005,139.580002,136.410004,139.110001,137.490005,5931900\n",
      "2019-08-08,138.449997,140.419998,137.759995,140.100006,140.100006,5261100\n",
      "2019-08-09,139.270004,139.309998,135.350006,136.130005,136.130005,5224000\n"
     ]
    }
   ],
   "source": [
    "# 显示后10行\n",
    "!cat data/ibm_stock.csv | tail -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date,Open,High,Low,Close,Adj Close,Volume\n",
      "2018-08-13,144.149994,144.300003,142.529999,142.710007,136.054092,2858500\n",
      "2018-08-14,143.000000,143.820007,142.929993,143.360001,136.673767,2860000\n",
      "2018-08-15,142.550003,144.000000,142.000000,143.910004,137.198105,4241500\n",
      "2018-08-16,144.369995,145.679993,144.369995,145.339996,138.561432,5250700\n",
      "2018-08-17,144.789993,146.389999,144.690002,146.059998,139.247864,2678600\n",
      "2018-08-20,146.369995,147.160004,146.149994,146.509995,139.676849,2499700\n",
      "2018-08-21,147.000000,147.119995,145.889999,145.970001,139.162048,3106900\n",
      "2018-08-22,146.009995,146.160004,145.190002,145.240005,138.466110,2389000\n",
      "2018-08-23,145.039993,145.520004,144.750000,145.369995,138.590012,2220400\n"
     ]
    }
   ],
   "source": [
    "# 显示前10行\n",
    "!cat data/ibm_stock.csv | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 编写脚本\n",
    "#### mapper\n",
    "\n",
    "将以下代码保存为stock_mapper.py文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    # 从标准输入读取数据\n",
    "    for line in sys.stdin:\n",
    "        # Date,Open,High,Low,Close,Adj Close,Volume\n",
    "        row = line.split(\",\")\n",
    "        try:\n",
    "            # 开盘价\n",
    "            open_price = float(row[1])\n",
    "            # 收盘价\n",
    "            close_price = float(row[-3])\n",
    "\n",
    "            # 计算价格变化百分比\n",
    "            price_change = ((open_price - close_price) / open_price )*100\n",
    "\n",
    "            percent = str(round(price_change, 2)) + \"%\"\n",
    "            print('{0}\\t{1}'.format(percent, 1))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reducer\n",
    "将以下代码保存为stock_reducer.py文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\t1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "def main():\n",
    "    # 定义变量存储 单词 和 词频\n",
    "    current_word = None \n",
    "    current_count = 1\n",
    "\n",
    "    for line in sys.stdin:\n",
    "        # 读取mapper函数输出的结果\n",
    "        word, count = line.strip().split(\"\\t\")\n",
    "\n",
    "        # 判断当前是否存单词\n",
    "        if current_word:\n",
    "            if word == current_word:\n",
    "                current_count += int(count)\n",
    "            else:\n",
    "                print('{0}\\t{1}'.format(current_word, current_count))\n",
    "                current_count = 1\n",
    "        # 赋值当前单词\n",
    "        current_word = word\n",
    "\n",
    "    # 处理最后一行数据\n",
    "    if current_count >= 1:\n",
    "        print('{0}\\t{1}'.format(current_word, current_count))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 本地测试\n",
    "\n",
    "在提交MapReduce Job之前必须用少量数据进行本地测试，以发现和修改程序错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gone_with_wind.txt pg20417.txt        pg5000.txt\n",
      "ibm_stock.csv      pg4300.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./stock_mapper.py ./stock_reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01%\t3\n",
      "-0.02%\t1\n",
      "-0.03%\t3\n",
      "-0.04%\t2\n",
      "-0.06%\t1\n",
      "-0.07%\t1\n",
      "-0.09%\t1\n",
      "-0.1%\t1\n",
      "-0.11%\t1\n",
      "-0.14%\t2\n",
      "-0.17%\t1\n",
      "-0.2%\t3\n"
     ]
    }
   ],
   "source": [
    "input_file = './data/ibm_stock.csv'\n",
    "mapper_script = './stock_mapper.py'\n",
    "reducer_script = './stock_reducer.py'\n",
    "!cat $input_file | $mapper_script | sort | $reducer_script | head -n 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 在hadoop集群上运行脚本，处理数据\n",
    "\n",
    "复制数据和脚本文件到hadoop集群master主机上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp ./data/ibm_stock.csv master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp ./stock_reducer.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp ./stock_mapper.py master:/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gone_with_wind.txt\n",
      "ibm_stock.csv\n",
      "jars\n",
      "stock_mapper.py\n",
      "stock_reducer.py\n",
      "word_count_mapper.py\n",
      "word_count_reducer.py\n"
     ]
    }
   ],
   "source": [
    "!docker exec master ls /root/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在master主机上传数据文件到hdfs中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker exec master hadoop fs -put /root/ibm_stock.csv /input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建MapReduce Job, 在hadoop集群上运行mapper和reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/08/12 11:53:44 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "19/08/12 11:53:44 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "19/08/12 11:53:44 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "19/08/12 11:53:44 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "19/08/12 11:53:44 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/08/12 11:53:44 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "19/08/12 11:53:45 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2035810303_0001\n",
      "19/08/12 11:53:45 INFO mapred.LocalDistributedCacheManager: Creating symlink: /tmp/hadoop-root/mapred/local/1565610825242/stock_mapper.py <- /usr/spark-2.3.0/stock_mapper.py\n",
      "19/08/12 11:53:45 INFO mapred.LocalDistributedCacheManager: Localized file:/root/stock_mapper.py as file:/tmp/hadoop-root/mapred/local/1565610825242/stock_mapper.py\n",
      "19/08/12 11:53:45 INFO mapred.LocalDistributedCacheManager: Creating symlink: /tmp/hadoop-root/mapred/local/1565610825243/stock_reducer.py <- /usr/spark-2.3.0/stock_reducer.py\n",
      "19/08/12 11:53:45 INFO mapred.LocalDistributedCacheManager: Localized file:/root/stock_reducer.py as file:/tmp/hadoop-root/mapred/local/1565610825243/stock_reducer.py\n",
      "19/08/12 11:53:45 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "19/08/12 11:53:45 INFO mapreduce.Job: Running job: job_local2035810303_0001\n",
      "19/08/12 11:53:45 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "19/08/12 11:53:45 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "19/08/12 11:53:45 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/12 11:53:45 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/12 11:53:45 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "19/08/12 11:53:45 INFO mapred.LocalJobRunner: Starting task: attempt_local2035810303_0001_m_000000_0\n",
      "19/08/12 11:53:45 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/12 11:53:45 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/12 11:53:45 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/12 11:53:45 INFO mapred.MapTask: Processing split: file:/input/ibm_stock.csv:0+18516\n",
      "19/08/12 11:53:45 INFO mapred.MapTask: numReduceTasks: 2\n",
      "19/08/12 11:53:46 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "19/08/12 11:53:46 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "19/08/12 11:53:46 INFO mapred.MapTask: soft limit at 83886080\n",
      "19/08/12 11:53:46 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "19/08/12 11:53:46 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "19/08/12 11:53:46 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "19/08/12 11:53:46 INFO streaming.PipeMapRed: PipeMapRed exec [/root/stock_mapper.py]\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "19/08/12 11:53:46 INFO mapreduce.Job: Job job_local2035810303_0001 running in uber mode : false\n",
      "19/08/12 11:53:46 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "19/08/12 11:53:46 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "19/08/12 11:53:46 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:46 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:46 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: Records R/W=250/1\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: \n",
      "19/08/12 11:53:47 INFO mapred.MapTask: Starting flush of map output\n",
      "19/08/12 11:53:47 INFO mapred.MapTask: Spilling map output\n",
      "19/08/12 11:53:47 INFO mapred.MapTask: bufstart = 0; bufend = 2105; bufvoid = 104857600\n",
      "19/08/12 11:53:47 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213400(104853600); length = 997/6553600\n",
      "19/08/12 11:53:47 INFO mapred.MapTask: Finished spill 0\n",
      "19/08/12 11:53:47 INFO mapred.Task: Task:attempt_local2035810303_0001_m_000000_0 is done. And is in the process of committing\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Records R/W=250/1\n",
      "19/08/12 11:53:47 INFO mapred.Task: Task 'attempt_local2035810303_0001_m_000000_0' done.\n",
      "19/08/12 11:53:47 INFO mapred.Task: Final Counters for attempt_local2035810303_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=153998\n",
      "\t\tFILE: Number of bytes written=513241\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=250\n",
      "\t\tMap output records=250\n",
      "\t\tMap output bytes=2105\n",
      "\t\tMap output materialized bytes=2617\n",
      "\t\tInput split bytes=77\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=250\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=187170816\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=18676\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local2035810303_0001_m_000000_0\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Starting task: attempt_local2035810303_0001_r_000000_0\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "19/08/12 11:53:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/12 11:53:47 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/12 11:53:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/12 11:53:47 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1f403ea8\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/12 11:53:47 INFO reduce.EventFetcher: attempt_local2035810303_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/08/12 11:53:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2035810303_0001_m_000000_0 decomp: 1295 len: 1299 to MEMORY\n",
      "19/08/12 11:53:47 INFO reduce.InMemoryMapOutput: Read 1295 bytes from map-output for attempt_local2035810303_0001_m_000000_0\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1295, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1295\n",
      "19/08/12 11:53:47 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/12 11:53:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/12 11:53:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1286 bytes\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: Merged 1 segments, 1295 bytes to disk to satisfy reduce memory limit\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: Merging 1 files, 1299 bytes from disk\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/12 11:53:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/12 11:53:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1286 bytes\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: PipeMapRed exec [/root/stock_reducer.py]\n",
      "19/08/12 11:53:47 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/08/12 11:53:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: Records R/W=123/1\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/12 11:53:47 INFO mapred.Task: Task:attempt_local2035810303_0001_r_000000_0 is done. And is in the process of committing\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/12 11:53:47 INFO mapred.Task: Task attempt_local2035810303_0001_r_000000_0 is allowed to commit now\n",
      "19/08/12 11:53:47 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/08/12 11:53:47 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2035810303_0001_r_000000_0' to file:/output/ibm_stock_data/_temporary/0/task_local2035810303_0001_r_000000\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Records R/W=123/1 > reduce\n",
      "19/08/12 11:53:47 INFO mapred.Task: Task 'attempt_local2035810303_0001_r_000000_0' done.\n",
      "19/08/12 11:53:47 INFO mapred.Task: Final Counters for attempt_local2035810303_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=157970\n",
      "\t\tFILE: Number of bytes written=515246\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=81\n",
      "\t\tReduce shuffle bytes=1299\n",
      "\t\tReduce input records=123\n",
      "\t\tReduce output records=81\n",
      "\t\tSpilled Records=123\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=187170816\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=706\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local2035810303_0001_r_000000_0\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Starting task: attempt_local2035810303_0001_r_000001_0\n",
      "19/08/12 11:53:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/08/12 11:53:47 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "19/08/12 11:53:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "19/08/12 11:53:47 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5b4e0d62\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "19/08/12 11:53:47 INFO reduce.EventFetcher: attempt_local2035810303_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "19/08/12 11:53:47 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local2035810303_0001_m_000000_0 decomp: 1314 len: 1318 to MEMORY\n",
      "19/08/12 11:53:47 INFO reduce.InMemoryMapOutput: Read 1314 bytes from map-output for attempt_local2035810303_0001_m_000000_0\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1314, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1314\n",
      "19/08/12 11:53:47 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "19/08/12 11:53:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/12 11:53:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1305 bytes\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: Merged 1 segments, 1314 bytes to disk to satisfy reduce memory limit\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: Merging 1 files, 1318 bytes from disk\n",
      "19/08/12 11:53:47 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "19/08/12 11:53:47 INFO mapred.Merger: Merging 1 sorted segments\n",
      "19/08/12 11:53:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1305 bytes\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: PipeMapRed exec [/root/stock_reducer.py]\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: Records R/W=127/1\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "19/08/12 11:53:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "19/08/12 11:53:47 INFO mapred.Task: Task:attempt_local2035810303_0001_r_000001_0 is done. And is in the process of committing\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "19/08/12 11:53:47 INFO mapred.Task: Task attempt_local2035810303_0001_r_000001_0 is allowed to commit now\n",
      "19/08/12 11:53:47 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2035810303_0001_r_000001_0' to file:/output/ibm_stock_data/_temporary/0/task_local2035810303_0001_r_000001\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Records R/W=127/1 > reduce\n",
      "19/08/12 11:53:47 INFO mapred.Task: Task 'attempt_local2035810303_0001_r_000001_0' done.\n",
      "19/08/12 11:53:47 INFO mapred.Task: Final Counters for attempt_local2035810303_0001_r_000001_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=160662\n",
      "\t\tFILE: Number of bytes written=517355\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=93\n",
      "\t\tReduce shuffle bytes=1318\n",
      "\t\tReduce input records=127\n",
      "\t\tReduce output records=93\n",
      "\t\tSpilled Records=127\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=10\n",
      "\t\tTotal committed heap usage (bytes)=187695104\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=791\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local2035810303_0001_r_000001_0\n",
      "19/08/12 11:53:47 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "19/08/12 11:53:48 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "19/08/12 11:53:48 INFO mapreduce.Job: Job job_local2035810303_0001 completed successfully\n",
      "19/08/12 11:53:48 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=472630\n",
      "\t\tFILE: Number of bytes written=1545842\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=250\n",
      "\t\tMap output records=250\n",
      "\t\tMap output bytes=2105\n",
      "\t\tMap output materialized bytes=2617\n",
      "\t\tInput split bytes=77\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=174\n",
      "\t\tReduce shuffle bytes=2617\n",
      "\t\tReduce input records=250\n",
      "\t\tReduce output records=174\n",
      "\t\tSpilled Records=500\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=10\n",
      "\t\tTotal committed heap usage (bytes)=562036736\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=18676\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1497\n",
      "19/08/12 11:53:48 INFO streaming.StreamJob: Output directory: /output/ibm_stock_data\n"
     ]
    }
   ],
   "source": [
    "jarPath = '/usr/hadoop-2.8.3/share/hadoop/tools/lib/'\n",
    "jarPath += 'hadoop-streaming-2.8.3.jar'\n",
    "jarArgs = '-files /root/stock_mapper.py,/root/stock_reducer.py \\\n",
    "-mapper /root/stock_mapper.py \\\n",
    "-reducer /root/stock_reducer.py \\\n",
    "-input /input/ibm_stock.csv \\\n",
    "-output /output/ibm_stock_data'\n",
    "# 启动两个任务执行MapReduce Job\n",
    "!docker exec master hadoop jar $jarPath -D mapreduce.job.reduces=2 $jarArgs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   1 root root          0 2019-08-12 11:49 /output/ibm_stock/_SUCCESS\n",
      "-rw-r--r--   1 root root       1465 2019-08-12 11:49 /output/ibm_stock/part-00000\n"
     ]
    }
   ],
   "source": [
    "!docker exec master hadoop fs -ls /output/ibm_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01%\t3\n",
      "-0.02%\t1\n",
      "-0.03%\t3\n",
      "-0.04%\t2\n",
      "-0.06%\t1\n",
      "-0.07%\t1\n",
      "-0.09%\t1\n",
      "-0.1%\t1\n",
      "-0.11%\t1\n",
      "-0.14%\t2\n",
      "-0.17%\t1\n",
      "-0.2%\t3\n",
      "-0.21%\t1\n",
      "-0.23%\t1\n",
      "-0.24%\t2\n",
      "-0.25%\t3\n",
      "-0.26%\t2\n",
      "-0.27%\t3\n",
      "-0.28%\t1\n",
      "-0.29%\t1\n"
     ]
    }
   ],
   "source": [
    "!docker exec master hadoop fs -cat /output/ibm_stock/part-00000 | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   1 root root          0 2019-08-12 11:53 /output/ibm_stock_data/_SUCCESS\n",
      "-rw-r--r--   1 root root        690 2019-08-12 11:53 /output/ibm_stock_data/part-00000\n",
      "-rw-r--r--   1 root root        775 2019-08-12 11:53 /output/ibm_stock_data/part-00001\n"
     ]
    }
   ],
   "source": [
    "!docker exec master hadoop fs -ls /output/ibm_stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.91%\t1\n",
      "2.03%\t1\n",
      "2.12%\t1\n",
      "2.25%\t1\n",
      "2.32%\t2\n",
      "2.49%\t1\n",
      "2.52%\t1\n",
      "3.02%\t1\n",
      "3.26%\t1\n",
      "3.68%\t1\n"
     ]
    }
   ],
   "source": [
    "data_file = '/output/ibm_stock_data/part-00001'\n",
    "!docker exec master hadoop fs -cat $data_file | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 使用matplotlib对处理结果进行可视化\n",
    "\n",
    "#### 下载处理结果到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_src = '/output/ibm_stock/part-00000'\n",
    "file_dst = '/root/ibm_stock_part-00000'\n",
    "!docker exec master hadoop fs -get $file_src $file_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker cp master:/root/ibm_stock_part-00000 ./data/ibm_stock_part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编写可视化代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max:  4.22\n",
      "Min:  -5.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 从MapReduce统计分析结果文件读取数据\n",
    "with open('data/ibm_stock_part-00000') as f:\n",
    "    # 存储 x轴和y轴 数据\n",
    "    x,y=[],[]\n",
    "\n",
    "    # 一行一行读取数据  数据格式: -1.7% 144\n",
    "    for line in f.readlines():\n",
    "        # 按照制表符分割\n",
    "        data=line.split(\"\\t\")\n",
    "        # 存储 每日百分比数据 -> X轴\n",
    "        x.append(float(data[0].strip('%')))\n",
    "        # 存储 统计数 -> Y轴\n",
    "        y.append(float(data[1]))\n",
    "    print(\"Max: \",max(x))\n",
    "    print(\"Min: \",min(x))\n",
    "\n",
    "    # 绘制柱状图\n",
    "    plt.bar(x,y, width=0.05)\n",
    "    # 显示\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
