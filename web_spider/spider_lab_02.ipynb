{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验2 基础爬虫实验\n",
    "\n",
    "##  爬取酷狗TOP500的数据\n",
    "\n",
    "### 1. 实验目的\n",
    "\n",
    "1. 熟悉requests和BeautifulSoup库的基本用法\n",
    "2. 掌握爬虫程序的开发流程\n",
    "3. 掌握如何使用requests抓取网页内容\n",
    "4. 掌握如何使用BeautifulSoup库解析HTML，采集目标数据\n",
    "5. 掌握如何利用分页规律生成分页网址并抓取多页内容\n",
    "6. 掌握如何爬取进入详细页面的链接网址，进而爬取详细页面中的目标数据\n",
    "7. 掌握如何将爬取到的数据保存为CSV文件\n",
    "\n",
    "### 2. 实验内容：\n",
    "\n",
    "本实验爬取的内容为[酷狗榜单中酷狗TOP500](http://www.kugou.com/yy/rank/home/1-8888.html)的音乐信息\n",
    "\n",
    "(1) 此站的分页规律如下（每页22首，共23页）：\n",
    "\n",
    "1. http://www.kugou.com/yy/rank/home/1-8888.html\n",
    "2. http://www.kugou.com/yy/rank/home/2-8888.html\n",
    "3. http://www.kugou.com/yy/rank/home/3-8888.html\n",
    "4. http://www.kugou.com/yy/rank/home/4-8888.html\n",
    "5. http://www.kugou.com/yy/rank/home/5-8888.html\n",
    "\n",
    "(2) 利用此分页规律爬取各分页中的音乐信息。\n",
    "\n",
    "(3) 需要爬取的信息有：排名、歌手、歌曲名和歌曲时长。如下图所示：\n",
    "\n",
    "![需获取的网页信息](images/kugou.jpeg)\n",
    "\n",
    "(4) 将爬取到的音乐信息存储到名为kugouTop100.csv文件中\n",
    "\n",
    "### 3. 实验代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myspider import HtmlParser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SougouTop500HtmlParser(HtmlParser):\n",
    "    '''\n",
    "    基于BeautifulSoup库的HTML解析器，用来解析搜狗TOP500网页数据\n",
    "    '''\n",
    "    def parse(self, html_content):\n",
    "        \n",
    "        # 内嵌函数，用于提取数据\n",
    "        def parse_data(html_content):\n",
    "            music_div = soup.find('div', class_='pc_temp_songlist')\n",
    "            music_list = music_div.find_all('li')\n",
    "            musics = []\n",
    "            for music_li in music_list:\n",
    "                rank = music_li.find('span', class_='pc_temp_num')\n",
    "                rank = rank.text.strip()\n",
    "                title = music_li.get('title')\n",
    "                artist = title.split('-')[0].strip()\n",
    "                title = title.split('-')[1].strip()\n",
    "                time = music_li.find('span', class_='pc_temp_time')\n",
    "                time = time.text.strip()\n",
    "                music_info = {'rank': rank, \n",
    "                              'artist': artist, \n",
    "                              'title': title, \n",
    "                              'time': time\n",
    "                             }\n",
    "                musics.append(music_info)\n",
    "            return musics\n",
    "        # 内嵌函数，用于提取URL（本站无需提取URL）\n",
    "        def parse_urls(html_content):\n",
    "            pass\n",
    "        \n",
    "        if html_content is None:\n",
    "            return None\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # 提取数据\n",
    "        data = parse_data(html_content)\n",
    "        # 提取URL\n",
    "        urls = parse_urls(html_content)\n",
    "        \n",
    "        return urls, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.kugou.com/yy/rank/home/1-8888.html',\n",
       " 'http://www.kugou.com/yy/rank/home/2-8888.html',\n",
       " 'http://www.kugou.com/yy/rank/home/3-8888.html',\n",
       " 'http://www.kugou.com/yy/rank/home/4-8888.html',\n",
       " 'http://www.kugou.com/yy/rank/home/5-8888.html']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myspider import SpiderScheduler\n",
    "from myspider import DataWriter\n",
    "\n",
    "html_parser = SougouTop500HtmlParser()\n",
    "data_writer = DataWriter('kugouTop100.csv')\n",
    "spider = SpiderScheduler(html_parser,data_writer)\n",
    "start_urls = ['http://www.kugou.com/yy/rank/home/{0}-8888.html'.format(i) for i in range(1,6)]\n",
    "start_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取：http://www.kugou.com/yy/rank/home/3-8888.html...已完成1，剩余4.\n",
      "正在爬取：http://www.kugou.com/yy/rank/home/5-8888.html...已完成2，剩余3.\n",
      "正在爬取：http://www.kugou.com/yy/rank/home/1-8888.html...已完成3，剩余2.\n",
      "正在爬取：http://www.kugou.com/yy/rank/home/4-8888.html...已完成4，剩余1.\n",
      "正在爬取：http://www.kugou.com/yy/rank/home/2-8888.html...已完成5，剩余0.\n"
     ]
    }
   ],
   "source": [
    "spider.crawl(start_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank,artist,title,time\r",
      "\r\n",
      "45,Ersen0306、Yelaman,攀登 (男生原版),3:16\r",
      "\r\n",
      "46,花姐,狂浪,3:01\r",
      "\r\n",
      "47,陆虎,雪落下的声音,5:11\r",
      "\r\n",
      "48,CRITTY,不谓侠,4:26\r",
      "\r\n",
      "49,Ava Max,Sweet but Psycho,3:07\r",
      "\r\n",
      "50,王贰浪,像鱼,4:45\r",
      "\r\n",
      "51,向熙,我叫安琪拉,3:10\r",
      "\r\n",
      "52,莫斯满、老猫,野花香,3:31\r",
      "\r\n",
      "53,陈雪凝,绿色,4:29\r",
      "\r\n",
      "54,虎二,你一定要幸福,4:19\r",
      "\r\n",
      "55,浙音4811,拜拜,3:21\r",
      "\r\n",
      "56,半阳,一曲相思,2:48\r",
      "\r\n",
      "57,小曼,最远的你是我最近的爱,4:04\r",
      "\r\n",
      "58,王琪,万爱千恩,5:22\r",
      "\r\n",
      "59,大壮,伪装,5:01\r",
      "\r\n",
      "60,灿烈、吴世勋,We Young,3:01\r",
      "\r\n",
      "61,韩红、林俊杰,飞云之下,4:26\r",
      "\r\n",
      "62,Katie Sky,Monsters,3:38\r",
      "\r\n",
      "63,苏谭谭,爱过了也伤过了,4:16\r",
      "\r\n",
      "64,崔荣宰、朴智敏,다 들어줄게 (倾听你的所有),3:22\r",
      "\r\n",
      "65,孙艺琪、崔伟立,怎么爱都爱不够,4:06\r",
      "\r\n",
      "66,丁芙妮,只是太爱你,4:07\r",
      "\r\n",
      "89,娜美,醉仙美,3:21\r",
      "\r\n",
      "90,蒋雪儿,从前的我快乐过,3:21\r",
      "\r\n",
      "91,太一,Lutra,2:58\r",
      "\r\n",
      "92,徐聪,绝不会放过,4:13\r",
      "\r\n",
      "93,LUM!X,Monster,3:03\r",
      "\r\n",
      "94,李荣浩,年少有为,4:39\r",
      "\r\n",
      "95,丫丫如意,以前打扰了 以后不会了,4:45\r",
      "\r\n",
      "96,展展与罗罗,沙漠骆驼,5:38\r",
      "\r\n",
      "97,陈柯宇,生僻字,3:36\r",
      "\r\n",
      "98,Clean Bandit、Demi Lovato,Solo (Acoustic),3:44\r",
      "\r\n",
      "99,张一阳,เนื้อคู่ฉันอยู่ไหน,2:50\r",
      "\r\n",
      "100,球球,没有人爱,3:48\r",
      "\r\n",
      "101,任盈盈,多年以后,4:03\r",
      "\r\n",
      "102,Vicetone、Cozi Zuehlsdorff,Way Back,3:28\r",
      "\r\n",
      "103,摩登兄弟,光年之外 (Live),3:56\r",
      "\r\n",
      "104,永彬Ryan.B,再也没有 (Live),3:49\r",
      "\r\n",
      "105,等什么君,辞九门回忆,4:05\r",
      "\r\n",
      "106,T,Pain、Ne,3:36\r",
      "\r\n",
      "107,夏婉安,天黑了(Cause夜、萤火虫和你),3:00\r",
      "\r\n",
      "108,蓝心羽,寂寞烟火,4:12\r",
      "\r\n",
      "109,HITA,赤伶,4:26\r",
      "\r\n",
      "110,221小伙伴,遥远的你 (正式版),3:36\r",
      "\r\n",
      "1,画词戏子,把孤独当作晚餐,2:51\r",
      "\r\n",
      "2,Ersen0306,攀登 (抖音版),1:42\r",
      "\r\n",
      "3,王力宏、谭维维,缘分一道桥,4:06\r",
      "\r\n",
      "4,G.E.M.邓紫棋,来自天堂的魔鬼,4:05\r",
      "\r\n",
      "5,梁博,出现又离开 (Live),6:44\r",
      "\r\n",
      "6,海来阿木、阿呷拉古、曲比阿且,别知己,4:40\r",
      "\r\n",
      "7,陈家乐,暗里着迷,3:56\r",
      "\r\n",
      "8,大欢,多年以后,4:03\r",
      "\r\n",
      "9,G.E.M.邓紫棋,画 (Live Piano Session II),2:49\r",
      "\r\n",
      "10,魏新雨,余情未了,3:36\r",
      "\r\n",
      "11,TFBOYS,第一次告白,3:33\r",
      "\r\n",
      "12,LEE HI、B.I,누구 없소 (NO ONE),3:14\r",
      "\r\n",
      "13,薛之谦,这么久没见,4:55\r",
      "\r\n",
      "14,Ice Paper,心如止水,3:05\r",
      "\r\n",
      "15,王七七,我愿意平凡的陪在你身旁,2:29\r",
      "\r\n",
      "16,Uu,那女孩对我说 (完整版),4:40\r",
      "\r\n",
      "17,童珺,天下有情人 (国语版),2:21\r",
      "\r\n",
      "18,永彬Ryan.B,放个大招给你看,2:58\r",
      "\r\n",
      "19,杨胖雨,情深深雨濛濛,2:35\r",
      "\r\n",
      "20,执素兮,赤伶 (正式版),4:56\r",
      "\r\n",
      "21,王恰恰,撕夜,4:32\r",
      "\r\n",
      "22,孙艺琪,情火,3:30\r",
      "\r\n",
      "67,上河Lin、司南,盗将行,3:18\r",
      "\r\n",
      "68,小咪,即兴,3:32\r",
      "\r\n",
      "69,杨语莲、王天昊,爱到最后就是痛 (对唱版),4:03\r",
      "\r\n",
      "70,梦涵,17岁,4:02\r",
      "\r\n",
      "71,隔壁老樊,红色高跟鞋,4:02\r",
      "\r\n",
      "72,海来阿木,别知己,4:33\r",
      "\r\n",
      "73,王天戈,心安理得,4:29\r",
      "\r\n",
      "74,Jennie,SOLO,2:49\r",
      "\r\n",
      "75,孤独诗人,渡我不渡她,3:02\r",
      "\r\n",
      "76,小曼,只要你还需要我,4:13\r",
      "\r\n",
      "77,G.E.M.邓紫棋,差不多姑娘,3:50\r",
      "\r\n",
      "78,李俊佑、小潘潘(潘柚彤),宠坏,3:16\r",
      "\r\n",
      "79,张泽熙,那个女孩,3:40\r",
      "\r\n",
      "80,王琪,站着等你三千年,6:21\r",
      "\r\n",
      "81,阿桑,一直很安静,4:10\r",
      "\r\n",
      "82,于晴,心如止水,3:05\r",
      "\r\n",
      "83,龙梅子、老猫,都说,3:35\r",
      "\r\n",
      "84,Corki,下坠Falling,3:45\r",
      "\r\n",
      "85,徐婧,今夜我一个人醉,4:07\r",
      "\r\n",
      "86,叶嘉,你笑起来真好看,2:48\r",
      "\r\n",
      "87,叶炫清,归去来兮,3:58\r",
      "\r\n",
      "88,尚士达,生而为人,5:19\r",
      "\r\n",
      "23,The Rose,I Don't Know You,2:38\r",
      "\r\n",
      "24,半吨兄弟,爱情错觉,4:03\r",
      "\r\n",
      "25,陈雪凝,你的酒馆对我打了烊,4:11\r",
      "\r\n",
      "26,NCF,艾力,2:25\r",
      "\r\n",
      "27,潘玮柏、G.E.M.邓紫棋、艾热,攀登 (Live),4:11\r",
      "\r\n",
      "28,李昕融、樊桐舟、李凯稠,你笑起来真好看,2:52\r",
      "\r\n",
      "29,小咪,我走后,4:08\r",
      "\r\n",
      "30,鹿晗,世界末日,4:27\r",
      "\r\n",
      "31,焦迈奇,我的名字,4:11\r",
      "\r\n",
      "32,Jain,Lil Mama,2:38\r",
      "\r\n",
      "33,孟颖,黎明前的黑暗,2:13\r",
      "\r\n",
      "34,崔伟立,情火,3:28\r",
      "\r\n",
      "35,Alan Walker、Sabrina Carpenter、Farruko,On My Way,3:13\r",
      "\r\n",
      "36,是你大哥阿,孤独,5:12\r",
      "\r\n",
      "37,等什么君,赤伶,4:42\r",
      "\r\n",
      "38,丸子呦,广寒宫,3:32\r",
      "\r\n",
      "39,隔壁老樊,多想在平庸的生活拥抱你 (Live),4:29\r",
      "\r\n",
      "40,苏北北,来自天堂的魔鬼 (Live),1:55\r",
      "\r\n",
      "41,盛婕,嘿李兰妈妈,2:38\r",
      "\r\n",
      "42,蕾蕾的小麦霸们、张振轩,赢在江湖 (童声版),3:46\r",
      "\r\n",
      "43,王小帅,最近 (正式版),3:37\r",
      "\r\n",
      "44,小星星Aurora,坠落星空,3:56\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat kugouTop100.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module myspider:\n",
      "\n",
      "NAME\n",
      "    myspider - myspider：基础爬虫框架模块\n",
      "\n",
      "DESCRIPTION\n",
      "    基础爬虫框架主要包括五大模块，分别为：\n",
      "    * 爬虫调度器（SpiderScheduler）：主要负责协调其他四个模块的工作\n",
      "    * URL管理器（UrlManager）：负责管理URL链接\n",
      "    * HTML下载器（HtmlDownloader）：负责下载HTML网页\n",
      "    * HTML解析器（HtmlParser）：从下载的HTML网页中解析出新的URL链接和目标数据\n",
      "    * 数据存储器（DataWriter）：用于将解析出来的目标数据保存到文件或数据库中\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        DataWriter\n",
      "        HtmlDownloader\n",
      "        HtmlParser\n",
      "        SpiderScheduler\n",
      "        UrlManager\n",
      "    \n",
      "    class DataWriter(builtins.object)\n",
      "     |  DataWriter(filename)\n",
      "     |  \n",
      "     |  数据存储器：存储数据到文件或者数据库中\n",
      "     |  默认实现，存储为CSV格式\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, filename)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get(self)\n",
      "     |      获取数据\n",
      "     |      :return: list\n",
      "     |  \n",
      "     |  put(self, data)\n",
      "     |      存放数据\n",
      "     |      :param data:要存放的数据(list)\n",
      "     |  \n",
      "     |  save(self)\n",
      "     |      存储数据\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HtmlDownloader(builtins.object)\n",
      "     |  HTML下载器：下载网页HTML文本\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  download(self, url)\n",
      "     |      从指定的URL下载HTML文本内容\n",
      "     |      :param url: 被下载网页的URL\n",
      "     |      :return: str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HtmlParser(builtins.object)\n",
      "     |  HTML解析器（抽象类）：解析网页，提取URL和数据\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  parse(self, html_content)\n",
      "     |      解析网页内容，抽取URL和数据\n",
      "     |      :return: tuple(urls,data)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SpiderScheduler(builtins.object)\n",
      "     |  SpiderScheduler(html_parser=None, data_writer=None)\n",
      "     |  \n",
      "     |  爬虫调度器：协调上述四个模块，爬取指定的URL列表\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, html_parser=None, data_writer=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  crawl(self, start_urls)\n",
      "     |      从指定的初始URL列表中爬取数据\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UrlManager(builtins.object)\n",
      "     |  URL管理器：负责管理URL链接\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  add_new_url(self, url)\n",
      "     |      添加新的URL\n",
      "     |      :param url: 单个URL\n",
      "     |      :return: None\n",
      "     |  \n",
      "     |  add_new_urls(self, urls)\n",
      "     |      添加多个新的URL\n",
      "     |      :param url: 多个URL\n",
      "     |      :return: None\n",
      "     |  \n",
      "     |  get_new_url(self)\n",
      "     |      获取一个未爬取的URL\n",
      "     |      :return: str\n",
      "     |  \n",
      "     |  has_new_url(self)\n",
      "     |      判断是否有待取的URL\n",
      "     |      :return: True/False\n",
      "     |  \n",
      "     |  num_of_new_urls(self)\n",
      "     |      获取未爬取URL的数量\n",
      "     |      return: int\n",
      "     |  \n",
      "     |  num_of_used_urls(self)\n",
      "     |      获取已爬取URL的数量\n",
      "     |      return: int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FILE\n",
      "    /Users/xiaobai/Workspace/labs/python-samples/web_spider/myspider.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import myspider\n",
    "help(myspider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SpiderScheduler in module myspider object:\n",
      "\n",
      "class SpiderScheduler(builtins.object)\n",
      " |  SpiderScheduler(html_parser=None, data_writer=None)\n",
      " |  \n",
      " |  爬虫调度器：协调上述四个模块，爬取指定的URL列表\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, html_parser=None, data_writer=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  crawl(self, start_urls)\n",
      " |      从指定的初始URL列表中爬取数据\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 代码分析\n",
    "\n",
    "请在此处编写实验分析（双击编辑）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 实验总结\n",
    "\n",
    "请在此处编写实验总结（双击编辑）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
