{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 实战项目-基础爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "内容导航：\n",
    "\n",
    "1. 基础爬虫架构及运行流程\n",
    "2. URL管理器\n",
    "3. HTML下载器\n",
    "4. HTML解析器\n",
    "5. 数据存储器\n",
    "6. 爬虫调度器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 基础爬虫架构及运行流程\n",
    "\n",
    "### 爬虫架构\n",
    "\n",
    "基础爬虫框架主要包括五大模块，分别为**爬虫调度器**、**URL管理器**、**HTML下载器**、**HTML解析器**和**数据存储器**。如下图所示：\n",
    "\n",
    "![基础爬虫架构](images/spider-architecture.jpeg)\n",
    "\n",
    "### 爬虫框架各模块的功能\n",
    "\n",
    "爬虫框架五大模块的功能如下：\n",
    "\n",
    "* 爬虫调度器主要负责协调其他四个模块的工作\n",
    "* URL管理器负责管理URL链接，维护已经爬取的URL集合和未爬取的URL集合，提供获取新URL的接口\n",
    "* HTML下载器用于从URL管理器中获取未爬取的URL链接并下载HTML网页\n",
    "* HTML解析器用于从HTML下载器中获取已经下载的HTML网页，并从中解析出新的URL链接交给URL管理器，解析出目标数据则交给数据存储器\n",
    "* 数据存储器用于将HTML解析器解析出来的目标数据通过文件或者数据库的形式存储起来。\n",
    "\n",
    "### 爬虫框架的运行流程\n",
    "\n",
    "爬虫框架的动态运行流程如下图所示：\n",
    "\n",
    "![运行流程](images/spider-flow.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 URL管理器\n",
    "\n",
    "主要包括:\n",
    "\n",
    "* 已爬取的URL集合: used_urls: set\n",
    "* 未爬取的URL集合: new_urls: set\n",
    "\n",
    "主要功能：URL**去重**\n",
    "\n",
    "接口：\n",
    "\n",
    "* 判断是否有待取的URL，方法：has_new_url():True/False\n",
    "* 添加新的URL到未爬取的集合中，方法：add_new_url(url),add_new_urls(urls)\n",
    "* 获取一个未爬取的URL，方法：get_new_url():str\n",
    "* 获取未爬取URL集合的大小，方法：num_of_new_urls(): int\n",
    "* 获取已爬取URL集合的大小，方法：num_of_used_urls(): int\n",
    "\n",
    "实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrlManager:\n",
    "    '''URL管理器：负责管理URL链接'''\n",
    "    \n",
    "    # 构造方法\n",
    "    def __init__(self):\n",
    "        # 未爬取的URL集合\n",
    "        self.new_urls = set()\n",
    "        # 已爬取的URL集合\n",
    "        self.used_urls = set()\n",
    "        \n",
    "    def has_new_url(self):\n",
    "        '''\n",
    "        判断是否有待取的URL\n",
    "        :return: True/False\n",
    "        '''\n",
    "        return self.num_of_new_urls() != 0\n",
    "    \n",
    "    def add_new_url(self, url):\n",
    "        '''\n",
    "        添加新的URL\n",
    "        :param url: 单个URL\n",
    "        :return: None\n",
    "        '''\n",
    "        if url is None:\n",
    "            return\n",
    "        if url not in self.new_urls and url not in self.used_urls:\n",
    "            self.new_urls.add(url)\n",
    "    \n",
    "    def add_new_urls(self, urls):\n",
    "        '''\n",
    "        添加多个新的URL\n",
    "        :param url: 多个URL\n",
    "        :return: None\n",
    "        '''\n",
    "        if urls is None or len(urls) == 0:\n",
    "            return\n",
    "        for url in urls:\n",
    "            self.add_new_url(url)\n",
    "    \n",
    "    def get_new_url(self):\n",
    "        '''\n",
    "        获取一个未爬取的URL\n",
    "        :return: str\n",
    "        '''\n",
    "        url = self.new_urls.pop()\n",
    "        self.used_urls.add(url)\n",
    "        return url\n",
    "            \n",
    "    def num_of_new_urls(self):\n",
    "        '''\n",
    "        获取未爬取URL的数量\n",
    "        return: int\n",
    "        '''\n",
    "        return len(self.new_urls)\n",
    "    \n",
    "    def num_of_used_urls(self):\n",
    "        '''\n",
    "        获取已爬取URL的数量\n",
    "        return: int\n",
    "        '''\n",
    "        return len(self.used_urls)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 HTML下载器\n",
    "\n",
    "功能：下载网页\n",
    "\n",
    "接口：\n",
    "\n",
    "* 从指定的URL下载HTML页面，方法：download(url): str（html文本）\n",
    "\n",
    "实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class HtmlDownloader:\n",
    "    '''\n",
    "    HTML下载器：下载网页HTML文本\n",
    "    '''\n",
    "    def download(self, url):\n",
    "        '''\n",
    "        从指定的URL下载HTML文本内容\n",
    "        :return: str\n",
    "        '''\n",
    "        if url is None:\n",
    "            return None\n",
    "        user_agent = 'Mozilla/4.0 (compatible); MSIE 5.5; Windows NT)'\n",
    "        headers = {'User-Agent': user_agent}\n",
    "        r = requests.get(url, headers=headers)\n",
    "        if r.status_code == 200:\n",
    "            r.encoding = 'utf-8'\n",
    "            return r.text\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 HTML解析器\n",
    "\n",
    "### 父类（抽象）功能接口\n",
    "\n",
    "功能：解析网页，提取数据和链接\n",
    "\n",
    "接口：\n",
    "\n",
    "* 解析网页内容，抽取URL和数据，方法：parse(html_content):(urls, data)\n",
    "\n",
    "实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HtmlParser:\n",
    "    '''\n",
    "    HTML解析器（抽象类）：解析网页，提取URL和数据\n",
    "    '''\n",
    "    \n",
    "    def parse(self, html_content):\n",
    "        '''\n",
    "        解析网页内容，抽取URL和数据\n",
    "        :return :tuple(urls,data)\n",
    "        '''\n",
    "        raise NotImplementedError(\"必须重写该方法！\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具体类：搜狗Top500网页解析器\n",
    "\n",
    "实现技术：\n",
    "\n",
    "基于BeautifulSoup库的解析器，用来解析搜狗TOP500网页数据\n",
    "\n",
    "实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SougouTop500HtmlParser(HtmlParser):\n",
    "    '''\n",
    "    基于BeautifulSoup库的HTML解析器，用来解析搜狗TOP500网页数据\n",
    "    '''\n",
    "    def parse(self, html_content):\n",
    "        \n",
    "        # 内嵌函数，用于提取数据\n",
    "        def parse_data(html_content):\n",
    "            music_div = soup.find('div', class_='pc_temp_songlist')\n",
    "            music_list = music_div.find_all('li')\n",
    "            musics = []\n",
    "            for music_li in music_list:\n",
    "                rank = music_li.find('span', class_='pc_temp_num')\n",
    "                rank = rank.text.strip()\n",
    "                title = music_li.get('title')\n",
    "                artist = title.split('-')[0].strip()\n",
    "                title = title.split('-')[1].strip()\n",
    "                time = music_li.find('span', class_='pc_temp_time')\n",
    "                time = time.text.strip()\n",
    "                music_info = {'rank': rank, \n",
    "                              'artist': artist, \n",
    "                              'title': title, \n",
    "                              'time': time\n",
    "                             }\n",
    "                musics.append(music_info)\n",
    "            return musics\n",
    "        # 内嵌函数，用于提取URL（本站无需提取URL）\n",
    "        def parse_urls(html_content):\n",
    "            pass\n",
    "        \n",
    "        if html_content is None:\n",
    "            return None\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # 提取数据\n",
    "        data = parse_data(html_content)\n",
    "        # 提取URL\n",
    "        urls = parse_urls(html_content)\n",
    "        \n",
    "        return urls, data      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 数据存储器\n",
    "\n",
    "包含：数据缓冲区: data_buffer\n",
    "\n",
    "功能：存储数据到文件或者数据库中\n",
    "\n",
    "接口：\n",
    "\n",
    "* 存储数据集,方法：write(filename, data)\n",
    "\n",
    "实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class DataWriter:\n",
    "    '''\n",
    "    数据存储器：存储数据到文件或者数据库中\n",
    "    默认实现，存储为CSV格式\n",
    "    '''\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.data_buffer = []\n",
    "        \n",
    "    def put(self, data):\n",
    "        self.data_buffer.extend(data)\n",
    "    \n",
    "    def get(self):\n",
    "        return self.data_buffer\n",
    "        \n",
    "    def save(self):\n",
    "        with open(self.filename, 'a', encoding='utf-8') as f:\n",
    "            fieldnames = self.data_buffer[0]\n",
    "            dict_writer = csv.DictWriter(f, fieldnames)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(self.data_buffer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 爬虫调度器\n",
    "\n",
    "包含：\n",
    "\n",
    "* URL管理器对象\n",
    "* HTML下载器对象\n",
    "* HTML解析器对象\n",
    "* 数据存储器对象\n",
    "\n",
    "功能：协调上述四个模块，爬取指定的URL列表\n",
    "\n",
    "接口：\n",
    "\n",
    "* 爬取指定的URL列表：方法：crawl(start_urls)\n",
    "\n",
    "实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpiderScheduler:\n",
    "    '''\n",
    "    爬虫调度器：协调上述四个模块，爬取指定的URL列表\n",
    "    '''\n",
    "    # 构造方法\n",
    "    def __init__(self, html_parser = None, data_writer = None):\n",
    "        self.url_manager = UrlManager()\n",
    "        self.html_downloader = HtmlDownloader()\n",
    "        if html_parser is None:\n",
    "            self.html_parser = HtmlParser()\n",
    "        else:\n",
    "            self.html_parser = html_parser\n",
    "        if data_writer is None:\n",
    "            self.data_writer = DataWriter(\"spider_data.csv\")\n",
    "        else:\n",
    "            self.data_writer = data_writer\n",
    "    \n",
    "    def crawl(self, start_urls):\n",
    "        self.url_manager.add_new_urls(start_urls)\n",
    "        while(self.url_manager.has_new_url()):\n",
    "            new_url = self.url_manager.get_new_url()\n",
    "            print('正在爬取：{}'.format(new_url), end='...')\n",
    "            \n",
    "            try:\n",
    "                html_text = self.html_downloader.download(new_url)\n",
    "                urls, data = self.html_parser.parse(html_text)\n",
    "                if (urls is not None) and (len(urls)) > 0:\n",
    "                    self.url_manager.add_new_urls(urls)\n",
    "                if data is not None:\n",
    "                    self.data_writer.put(data)\n",
    "                num_of_used = self.url_manager.num_of_used_urls()\n",
    "                num_of_new = self.url_manager.num_of_new_urls()\n",
    "                print('已完成{0}，剩余{1}.'.format(num_of_used, num_of_new))                \n",
    "            except Exception as e:\n",
    "                print(\"发生错误：{}\".format(e))\n",
    "                raise e\n",
    "\n",
    "        self.data_writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function print in module builtins:\n",
      "\n",
      "print(...)\n",
      "    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n",
      "    \n",
      "    Prints the values to a stream, or to sys.stdout by default.\n",
      "    Optional keyword arguments:\n",
      "    file:  a file-like object (stream); defaults to the current sys.stdout.\n",
      "    sep:   string inserted between values, default a space.\n",
      "    end:   string appended after the last value, default a newline.\n",
      "    flush: whether to forcibly flush the stream.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取：http://www.kugou.com/yy/rank/home/1-8888.html...已完成1，剩余0.\n"
     ]
    }
   ],
   "source": [
    "spider = SpiderScheduler(html_parser = SougouTop500HtmlParser())\n",
    "spider.crawl(['http://www.kugou.com/yy/rank/home/1-8888.html'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm spider_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.kugou.com/yy/rank/home/1-8888.html',\n",
       " 'http://www.kugou.com/yy/rank/home/2-8888.html',\n",
       " 'http://www.kugou.com/yy/rank/home/3-8888.html',\n",
       " 'http://www.kugou.com/yy/rank/home/4-8888.html',\n",
       " 'http://www.kugou.com/yy/rank/home/5-8888.html']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myspider import SpiderScheduler\n",
    "\n",
    "spider = SpiderScheduler(html_parser = SougouTop500HtmlParser())\n",
    "start_urls = ['http://www.kugou.com/yy/rank/home/{0}-8888.html'.format(i) for i in range(1,6)]\n",
    "start_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在爬取：http://www.kugou.com/yy/rank/home/2-8888.html...已完成1，剩余4.\n",
      "正在爬取：http://www.kugou.com/yy/rank/home/4-8888.html...已完成2，剩余3.\n",
      "正在爬取：http://www.kugou.com/yy/rank/home/3-8888.html...已完成3，剩余2.\n",
      "正在爬取：http://www.kugou.com/yy/rank/home/1-8888.html...已完成4，剩余1.\n",
      "正在爬取：http://www.kugou.com/yy/rank/home/5-8888.html...已完成5，剩余0.\n"
     ]
    }
   ],
   "source": [
    "spider.crawl(start_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank,artist,title,time\r",
      "\r\n",
      "23,The Rose,I Don't Know You,2:38\r",
      "\r\n",
      "24,半吨兄弟,爱情错觉,4:03\r",
      "\r\n",
      "25,陈雪凝,你的酒馆对我打了烊,4:11\r",
      "\r\n",
      "26,NCF,艾力,2:25\r",
      "\r\n",
      "27,潘玮柏、G.E.M.邓紫棋、艾热,攀登 (Live),4:11\r",
      "\r\n",
      "28,李昕融、樊桐舟、李凯稠,你笑起来真好看,2:52\r",
      "\r\n",
      "29,小咪,我走后,4:08\r",
      "\r\n",
      "30,鹿晗,世界末日,4:27\r",
      "\r\n",
      "31,焦迈奇,我的名字,4:11\r",
      "\r\n",
      "32,Jain,Lil Mama,2:38\r",
      "\r\n",
      "33,孟颖,黎明前的黑暗,2:13\r",
      "\r\n",
      "34,崔伟立,情火,3:28\r",
      "\r\n",
      "35,Alan Walker、Sabrina Carpenter、Farruko,On My Way,3:13\r",
      "\r\n",
      "36,是你大哥阿,孤独,5:12\r",
      "\r\n",
      "37,等什么君,赤伶,4:42\r",
      "\r\n",
      "38,丸子呦,广寒宫,3:32\r",
      "\r\n",
      "39,隔壁老樊,多想在平庸的生活拥抱你 (Live),4:29\r",
      "\r\n",
      "40,苏北北,来自天堂的魔鬼 (Live),1:55\r",
      "\r\n",
      "41,盛婕,嘿李兰妈妈,2:38\r",
      "\r\n",
      "42,蕾蕾的小麦霸们、张振轩,赢在江湖 (童声版),3:46\r",
      "\r\n",
      "43,王小帅,最近 (正式版),3:37\r",
      "\r\n",
      "44,小星星Aurora,坠落星空,3:56\r",
      "\r\n",
      "67,上河Lin、司南,盗将行,3:18\r",
      "\r\n",
      "68,小咪,即兴,3:32\r",
      "\r\n",
      "69,杨语莲、王天昊,爱到最后就是痛 (对唱版),4:03\r",
      "\r\n",
      "70,梦涵,17岁,4:02\r",
      "\r\n",
      "71,隔壁老樊,红色高跟鞋,4:02\r",
      "\r\n",
      "72,海来阿木,别知己,4:33\r",
      "\r\n",
      "73,王天戈,心安理得,4:29\r",
      "\r\n",
      "74,Jennie,SOLO,2:49\r",
      "\r\n",
      "75,孤独诗人,渡我不渡她,3:02\r",
      "\r\n",
      "76,小曼,只要你还需要我,4:13\r",
      "\r\n",
      "77,G.E.M.邓紫棋,差不多姑娘,3:50\r",
      "\r\n",
      "78,李俊佑、小潘潘(潘柚彤),宠坏,3:16\r",
      "\r\n",
      "79,张泽熙,那个女孩,3:40\r",
      "\r\n",
      "80,王琪,站着等你三千年,6:21\r",
      "\r\n",
      "81,阿桑,一直很安静,4:10\r",
      "\r\n",
      "82,于晴,心如止水,3:05\r",
      "\r\n",
      "83,龙梅子、老猫,都说,3:35\r",
      "\r\n",
      "84,Corki,下坠Falling,3:45\r",
      "\r\n",
      "85,徐婧,今夜我一个人醉,4:07\r",
      "\r\n",
      "86,叶嘉,你笑起来真好看,2:48\r",
      "\r\n",
      "87,叶炫清,归去来兮,3:58\r",
      "\r\n",
      "88,尚士达,生而为人,5:19\r",
      "\r\n",
      "45,Ersen0306、Yelaman,攀登 (男生原版),3:16\r",
      "\r\n",
      "46,花姐,狂浪,3:01\r",
      "\r\n",
      "47,陆虎,雪落下的声音,5:11\r",
      "\r\n",
      "48,CRITTY,不谓侠,4:26\r",
      "\r\n",
      "49,Ava Max,Sweet but Psycho,3:07\r",
      "\r\n",
      "50,王贰浪,像鱼,4:45\r",
      "\r\n",
      "51,向熙,我叫安琪拉,3:10\r",
      "\r\n",
      "52,莫斯满、老猫,野花香,3:31\r",
      "\r\n",
      "53,陈雪凝,绿色,4:29\r",
      "\r\n",
      "54,虎二,你一定要幸福,4:19\r",
      "\r\n",
      "55,浙音4811,拜拜,3:21\r",
      "\r\n",
      "56,半阳,一曲相思,2:48\r",
      "\r\n",
      "57,小曼,最远的你是我最近的爱,4:04\r",
      "\r\n",
      "58,王琪,万爱千恩,5:22\r",
      "\r\n",
      "59,大壮,伪装,5:01\r",
      "\r\n",
      "60,灿烈、吴世勋,We Young,3:01\r",
      "\r\n",
      "61,韩红、林俊杰,飞云之下,4:26\r",
      "\r\n",
      "62,Katie Sky,Monsters,3:38\r",
      "\r\n",
      "63,苏谭谭,爱过了也伤过了,4:16\r",
      "\r\n",
      "64,崔荣宰、朴智敏,다 들어줄게 (倾听你的所有),3:22\r",
      "\r\n",
      "65,孙艺琪、崔伟立,怎么爱都爱不够,4:06\r",
      "\r\n",
      "66,丁芙妮,只是太爱你,4:07\r",
      "\r\n",
      "1,画词戏子,把孤独当作晚餐,2:51\r",
      "\r\n",
      "2,Ersen0306,攀登 (抖音版),1:42\r",
      "\r\n",
      "3,王力宏、谭维维,缘分一道桥,4:06\r",
      "\r\n",
      "4,G.E.M.邓紫棋,来自天堂的魔鬼,4:05\r",
      "\r\n",
      "5,梁博,出现又离开 (Live),6:44\r",
      "\r\n",
      "6,海来阿木、阿呷拉古、曲比阿且,别知己,4:40\r",
      "\r\n",
      "7,陈家乐,暗里着迷,3:56\r",
      "\r\n",
      "8,大欢,多年以后,4:03\r",
      "\r\n",
      "9,G.E.M.邓紫棋,画 (Live Piano Session II),2:49\r",
      "\r\n",
      "10,魏新雨,余情未了,3:36\r",
      "\r\n",
      "11,TFBOYS,第一次告白,3:33\r",
      "\r\n",
      "12,LEE HI、B.I,누구 없소 (NO ONE),3:14\r",
      "\r\n",
      "13,薛之谦,这么久没见,4:55\r",
      "\r\n",
      "14,Ice Paper,心如止水,3:05\r",
      "\r\n",
      "15,王七七,我愿意平凡的陪在你身旁,2:29\r",
      "\r\n",
      "16,Uu,那女孩对我说 (完整版),4:40\r",
      "\r\n",
      "17,童珺,天下有情人 (国语版),2:21\r",
      "\r\n",
      "18,永彬Ryan.B,放个大招给你看,2:58\r",
      "\r\n",
      "19,杨胖雨,情深深雨濛濛,2:35\r",
      "\r\n",
      "20,执素兮,赤伶 (正式版),4:56\r",
      "\r\n",
      "21,王恰恰,撕夜,4:32\r",
      "\r\n",
      "22,孙艺琪,情火,3:30\r",
      "\r\n",
      "89,娜美,醉仙美,3:21\r",
      "\r\n",
      "90,蒋雪儿,从前的我快乐过,3:21\r",
      "\r\n",
      "91,太一,Lutra,2:58\r",
      "\r\n",
      "92,徐聪,绝不会放过,4:13\r",
      "\r\n",
      "93,LUM!X,Monster,3:03\r",
      "\r\n",
      "94,李荣浩,年少有为,4:39\r",
      "\r\n",
      "95,丫丫如意,以前打扰了 以后不会了,4:45\r",
      "\r\n",
      "96,展展与罗罗,沙漠骆驼,5:38\r",
      "\r\n",
      "97,陈柯宇,生僻字,3:36\r",
      "\r\n",
      "98,Clean Bandit、Demi Lovato,Solo (Acoustic),3:44\r",
      "\r\n",
      "99,张一阳,เนื้อคู่ฉันอยู่ไหน,2:50\r",
      "\r\n",
      "100,球球,没有人爱,3:48\r",
      "\r\n",
      "101,任盈盈,多年以后,4:03\r",
      "\r\n",
      "102,Vicetone、Cozi Zuehlsdorff,Way Back,3:28\r",
      "\r\n",
      "103,摩登兄弟,光年之外 (Live),3:56\r",
      "\r\n",
      "104,永彬Ryan.B,再也没有 (Live),3:49\r",
      "\r\n",
      "105,等什么君,辞九门回忆,4:05\r",
      "\r\n",
      "106,T,Pain、Ne,3:36\r",
      "\r\n",
      "107,夏婉安,天黑了(Cause夜、萤火虫和你),3:00\r",
      "\r\n",
      "108,蓝心羽,寂寞烟火,4:12\r",
      "\r\n",
      "109,HITA,赤伶,4:26\r",
      "\r\n",
      "110,221小伙伴,遥远的你 (正式版),3:36\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat spider_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
